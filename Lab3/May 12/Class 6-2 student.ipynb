{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS470 Introduction to Artificial Intelligence\n",
    "## Deep Learning Practice \n",
    "#### TA. Yechan Hwang\n",
    "---\n",
    "\n",
    "### Agenda for this practice\n",
    "#### 1. Weather dataset\n",
    "#### 2. Univariate model\n",
    "#### 3. Normalizing the data\n",
    "#### 4. Multivariate model\n",
    "#### 5. Multi-step model\n",
    "---\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6-2. Time series forecasting \n",
    "In this practice, we will use two approaches for time series forecasting.\n",
    "- First, we will forecast a **univariate** time series\n",
    "- Then we will forecast a **multivariate** time series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "mpl.rcParams['figure.figsize'] = (8, 6)\n",
    "mpl.rcParams['axes.grid'] = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The weather dataset\n",
    "This practice uses a [`weather time series dataset`](https://www.bgc-jena.mpg.de/wetter/) (recorded by the Max-Planck-Institute for Biogeochemistry).\n",
    "\n",
    "This dataset contains 14 different features such as air temperature, atmospheric pressure, and humidity. These were collected every 10 minutes, beginning in 2003. For efficiency, we will use only the data collected between 2009 and 2016. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/jena_climate_2009_2016.csv.zip\n",
      "13574144/13568290 [==============================] - 1s 0us/step\n"
     ]
    }
   ],
   "source": [
    "zip_path = tf.keras.utils.get_file(\n",
    "    origin='https://storage.googleapis.com/tensorflow/tf-keras-datasets/jena_climate_2009_2016.csv.zip',\n",
    "    fname='jena_climate_2009_2016.csv.zip',\n",
    "    extract=True)\n",
    "csv_path, _ = os.path.splitext(zip_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>\n",
    "The data is in the form of CSV. If you are unfamiliar with handling the CSV data well in Tensorflow, check this tutorial:\n",
    "\n",
    "[link](https://www.tensorflow.org/tutorials/load_data/pandas_dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a glance at the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see above, an observation is recorded every 10 mintues from 01/01/2009. This means that, you will have 6 observations for a single hour. Similarly, a single day will contain 144 (6x24) observations.\n",
    "\n",
    "For this prediction problem, you can specify any time period to predict. Let's say **you want to predict the temperature for 6 hours in the future**. And in order to make this prediction, let's say that **you decided to use 5 days of observations**. Then you can create a window containing the last 720 (5x144) observations to train the model and predict 6 hours in the future. Many such configurations are possible and you can choose best one after experiments with various input/output shapes.\n",
    "\n",
    "<img src=\"images/example_model.png\" alt=\"Drawing\" style=\"width: 1100px;\"/>\n",
    "\n",
    "<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting seed to ensure reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(13)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Forecast a univariate time series\n",
    "First, we will train a model that learns **using only a single feature (temperature)** and use this model to make predictions for temperature values in the future.\n",
    "\n",
    "Let's first extract only the temperature from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uni_data = \n",
    "uni_data.index = \n",
    "\n",
    "print(uni_data.head())\n",
    "print(uni_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>\n",
    "\n",
    "Let's observe how this data looks across time. By the `plot` function, we can visualize the temperature data.\n",
    "\n",
    "First we will check global trend of temperature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uni_data.plot(subplots=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also we can focus on specific period (from index 20000 to 21000)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(uni_data[20000:21000]).plot(subplots=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By using `Series.values`, we convert pandas data to numpy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uni_data = uni_data.values\n",
    "uni_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In both the following practices, we will use the first 300,000 rows of the data as training dataset and the remaining will be the validation dataset.\n",
    "\n",
    "This amounts to ~3,000,000 minutes = ~2100 days worth of training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_SPLIT = 300000\n",
    "training_data = uni_data[:TRAIN_SPLIT]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is important to **normalize features before training a neural network** since scales for each feature can be different significantly. A common way to do so is by subtracting the mean and dividing by the standard deviation of each feature.\n",
    "\n",
    "And notice that the **mean and standard deviation should be computed using only the training data**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data_mean = \n",
    "training_data_std = \n",
    "print(training_data_mean,training_data_std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's normalize the data and visualize them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_uni_data = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(norm_uni_data).plot(subplots=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Univariate model\n",
    "\n",
    "Let's now **create the data for the univariate model**. In this section, **the model will be given the last 20 recorded temperature observations** and learn to **predict the temperature of the next time step**.\n",
    "\n",
    "<br/>\n",
    "\n",
    "<img src=\"images/approach.png\" alt=\"Drawing\" style=\"width: 800px;\"/>\n",
    "\n",
    "<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The helper function below returns the specific windows of time for the model to train on. The parameter `history_size` is the size of the window of past information. So our model will use the number of `history_size` information for prediction. The `target_size` defines how far in the future does the model need to learn to predict. In other words, if the `target_size` 0 is the above model, it will predict the temperature for time step 21 and it will predict the temparature for time step 22 if the `target_size` is 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def univariate_data(dataset, start_index, end_index, history_size, target_size):\n",
    "    data = []\n",
    "    labels = []\n",
    "    \n",
    "    start_index = start_index + history_size\n",
    "    if end_index is None:\n",
    "        end_index = len(dataset)\n",
    "    \n",
    "    for i in range(start_index, end_index- target_size):\n",
    "        indices = range(i-history_size, i)\n",
    "        # Reshape data from (history_size,) to (history_size, 1)\n",
    "        data.append(np.reshape(dataset[indices], (history_size, 1)))\n",
    "        labels.append(dataset[i+target_size])\n",
    "    return np.array(data), np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "univariate_past_history = 20\n",
    "univariate_future_target = 0\n",
    "\n",
    "# Create training dataset with univariate feature\n",
    "x_train_uni, y_train_uni = univariate_data(norm_uni_data, 0, TRAIN_SPLIT,\n",
    "                                           univariate_past_history,\n",
    "                                           univariate_future_target)\n",
    "\n",
    "x_val_uni, y_val_uni = univariate_data(norm_uni_data, TRAIN_SPLIT, None,\n",
    "                                       univariate_past_history,\n",
    "                                       univariate_future_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x_train_uni.shape)\n",
    "print(x_val_uni.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_uni_data[:25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print ('Single window of past history')\n",
    "print (x_train_uni[0])\n",
    "print ('\\n Target temperature to predict')\n",
    "print (y_train_uni[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/><br/>\n",
    "\n",
    "Now that the data has been created, let's take a look at a single example. The information given to the network is given in blue, and it must predict the value at the red cross."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_plot(plot_data, delta, title):\n",
    "    labels = ['History', 'True Future', 'Model Prediction']\n",
    "    marker = ['.-', 'rx', 'go']\n",
    "    time_steps = range(-plot_data[0].shape[0],0)\n",
    "    if delta:\n",
    "        future = delta\n",
    "    else:\n",
    "        future = 0\n",
    "\n",
    "    plt.title(title)\n",
    "    for i, x in enumerate(plot_data):\n",
    "        if i:\n",
    "            plt.plot(future, plot_data[i], marker[i], markersize=10,\n",
    "                label=labels[i])\n",
    "        else:\n",
    "            plt.plot(time_steps, plot_data[i].flatten(), marker[i], label=labels[i])\n",
    "    plt.legend()\n",
    "    plt.xlim([time_steps[0], (future+5)*2])\n",
    "    plt.xlabel('Time-Step')\n",
    "    return plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_plot([x_train_uni[0], y_train_uni[0]], 0, 'Sample Example')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Baseline\n",
    "Before training our model, let's first **construct a simple baseline model**. Baseline model will looks at all the history observations and **predicts the next point to be the average them**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def baseline(history):\n",
    "    return np.mean(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_plot([x_train_uni[0], y_train_uni[0], baseline(x_train_uni[0])], 0,\n",
    "           'Baseline Prediction Example')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see if our model can beat this baseline using a recurrent neural network after training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Recurrent neural network\n",
    "\n",
    "In this practical, we will use a specialized RNN layer Long Short Tem Memory (LSTM).\n",
    "Similar to RNN and GRU, LSTMs process a time series step-by-step, maintaining an internal state summarizing the information they've seen so far.\n",
    "\n",
    "Let's now use `tf.data` to shuffle, make batch, and cache the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 256\n",
    "BUFFER_SIZE = 10000\n",
    "\n",
    "train_univariate = tf.data.Dataset.from_tensor_slices((x_train_uni, y_train_uni))\n",
    "train_univariate = train_univariate.cache().shuffle(BUFFER_SIZE).batch(BATCH_SIZE).repeat()\n",
    "\n",
    "val_univariate = tf.data.Dataset.from_tensor_slices((x_val_uni, y_val_uni))\n",
    "val_univariate = val_univariate.batch(BATCH_SIZE).repeat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for elem in train_univariate.take(1):\n",
    "    print(elem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will make `simple_lstm_model` that has one LSTM with 8 units and one Dense layer with 1 unit. Also, since this problem is regression, we will use mean absolute error loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_lstm_model = tf.keras.Sequential([\n",
    "    \n",
    "\n",
    "])\n",
    "\n",
    "simple_lstm_model.compile(optimizer='adam', loss=    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make a sample prediction, to check the output of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x, y in val_univariate.take(1):\n",
    "    print(simple_lstm_model.predict(x).shape) # (BATCH_SIZE, Predicted_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train the model now. Due to the large size of the dataset, in the interest of saving time, each epoch will only run for 200 steps, instead of the complete training data as normally done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EVALUATION_INTERVAL = 200\n",
    "EPOCHS = 10\n",
    "\n",
    "simple_lstm_model.fit(   __data__   , epochs=  ,\n",
    "                      steps_per_epoch=  ,\n",
    "                      validation_data=  , validation_steps=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predict using the simple LSTM model\n",
    "Now that you have trained your simple LSTM, let's try and make a few predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x, y in val_univariate.take(3):\n",
    "    plot = show_plot([x[0].numpy(), y[0].numpy(),\n",
    "                      simple_lstm_model.predict(x)[0]], 0, 'Simple LSTM model')\n",
    "    plot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have seen the basics, let's move on to part two, where you will work with a multivariate time series."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Forecast a multivariate time series\n",
    "The original dataset contains **fourteen features**. For simplicity, this part **considers only three of the original fourteen**. The features used are air temperature, atmospheric pressure, and air density.\n",
    "\n",
    "To use more features, add their names to this list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_considered = ['p (mbar)', 'T (degC)', 'rho (g/m**3)']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = \n",
    "features.index = \n",
    "features.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at how each of these features vary across time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features.plot(subplots=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned, the **first step will be to normalize the dataset** using the mean and standard deviation of the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = \n",
    "data_mean = \n",
    "data_std ="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Single step multivariate model\n",
    "In a single step setup, **the model learns to predict a single point** in the future based on some history provided with multiple features.\n",
    "\n",
    "The below function performs the same windowing task as above but now with many features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multivariate_data(dataset, target, start_index, end_index, history_size,\n",
    "                      target_size, single_step=False):\n",
    "    data = []\n",
    "    labels = []\n",
    "\n",
    "    start_index = start_index + history_size\n",
    "    if end_index is None:\n",
    "        end_index = len(dataset) - target_size\n",
    "\n",
    "    for i in range(start_index, end_index):\n",
    "        indices = range(i-history_size, i, 1)\n",
    "        data.append(dataset[indices])\n",
    "        \n",
    "        \n",
    "        if single_step:\n",
    "            labels.append(target[i+target_size])\n",
    "        else:\n",
    "            labels.append(target[i:i+target_size])\n",
    "\n",
    "    return np.array(data), np.array(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this practice, the network will be given past 20 observations, but now each observations have three different features. Therefore, the amount of information the model can refer when making predictions has increased. But at the same time, the amount of information the model has to learn during training has also increased.\n",
    "\n",
    "Similar to what we did above, our model will be given the last 20 recorded temperature observations and learn to predict the temperature of the next time step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "past_history = 20\n",
    "future_target = 0\n",
    "\n",
    "x_train_single, y_train_single = multivariate_data(\n",
    "\n",
    ")\n",
    "\n",
    "x_val_single, y_val_single = multivariate_data(\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at a single data-point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 20 observations with 3 features \n",
    "print ('Single window of past history : {}'.format(x_train_single[0].shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_single = tf.data.Dataset.from_tensor_slices((x_train_single, y_train_single))\n",
    "train_data_single = train_data_single.cache().shuffle(BUFFER_SIZE).batch(BATCH_SIZE).repeat()\n",
    "\n",
    "val_data_single = tf.data.Dataset.from_tensor_slices((x_val_single, y_val_single))\n",
    "val_data_single = val_data_single.batch(BATCH_SIZE).repeat()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will make `single_step_model` that has one LSTM with 32 units and one Dense layer with 1 unit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_step_model = tf.keras.models.Sequential([\n",
    "\n",
    "\n",
    "])\n",
    "\n",
    "# mae : mean absolute error\n",
    "single_step_model.compile(optimizer='rmsprop', loss='mae')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will train our model and plot the training and validation loss on the graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EVALUATION_INTERVAL = 200\n",
    "EPOCHS = 10\n",
    "\n",
    "single_step_history = single_step_model.fit(\n",
    "\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_train_history(history, title):\n",
    "    loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "\n",
    "    epochs = range(len(loss))\n",
    "\n",
    "    plt.figure()\n",
    "\n",
    "    plt.plot(epochs, loss, 'b', label='Training loss')\n",
    "    plt.plot(epochs, val_loss, 'r', label='Validation loss')\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_train_history(single_step_history,\n",
    "                   'Single Step Training and validation loss')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predict a single step future\n",
    "Now that the model is trained, let's make a few sample predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x, y in val_data_single.take(3):\n",
    "    plot = show_plot([x[0][:, 1].numpy(), y[0].numpy(),\n",
    "                    single_step_model.predict(x)[0]], 0,\n",
    "                   'Single Step Prediction')\n",
    "    \n",
    "    plot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we compare our model's prediction with the univariate model, it seems that our model makes predictions better.\n",
    "\n",
    "##### Will increasing the number of features always increase performance?\n",
    "\n",
    "It depends on your dataset. While some features have very useful information for your model's prediction, some features may not. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although there are various methods for choosing which features to use (e.g. using the feature's statistics, PCA, embedding, etc.), we will not cover them in this practice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/><br/>\n",
    "\n",
    "#### Multi-step prediction model\n",
    "\n",
    "In a multi-step prediction model, given a past history, the model needs to learn to **predict a range of future values**. Thus, unlike a single step model, where only a single future point is predicted, **a multi-step model predict a sequence of the future**.\n",
    "\n",
    "For the multi-step model, the training data again consists of past 20 observations that have three different features. However, now the model needs to learn to predict the temperature **for the next 3 time steps**. For this task, the dataset needs to be preprocessed again so that each target object has 3 labels.\n",
    "\n",
    "\n",
    "<br/><br/>\n",
    "<img src=\"images/multi.png\" alt=\"Drawing\" style=\"width: 800px;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "past_history = 20\n",
    "future_target = 3\n",
    "\n",
    "x_train_multi, y_train_multi = multivariate_data(dataset, dataset[:, 1], 0,\n",
    "                                                 TRAIN_SPLIT, past_history,\n",
    "                                                 future_target, single_step=False)\n",
    "x_val_multi, y_val_multi = multivariate_data(dataset, dataset[:, 1],\n",
    "                                             TRAIN_SPLIT, None, past_history,\n",
    "                                             future_target, single_step=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check out a sample data-point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print ('Single window of past history : {}'.format(x_train_multi[0].shape))\n",
    "print ('\\nTarget temperature to predict : {}'.format(y_train_multi[0].shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_multi = tf.data.Dataset.from_tensor_slices((x_train_multi, y_train_multi))\n",
    "train_data_multi = train_data_multi.cache().shuffle(BUFFER_SIZE).batch(BATCH_SIZE).repeat()\n",
    "\n",
    "val_data_multi = tf.data.Dataset.from_tensor_slices((x_val_multi, y_val_multi))\n",
    "val_data_multi = val_data_multi.batch(BATCH_SIZE).repeat()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`multi_step_plot` is a helper function that visualizes many True/Predicted points on the plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_step_plot(history, true_future, prediction):\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    num_in = range(-len(history),0)\n",
    "    num_out = len(true_future)\n",
    "\n",
    "    plt.plot(num_in, np.array(history[:, 1]), label='History')\n",
    "    plt.plot(np.arange(num_out), np.array(true_future), 'bo',\n",
    "           label='True Future')\n",
    "    if prediction.any():\n",
    "        plt.plot(np.arange(num_out), np.array(prediction), 'ro',\n",
    "                 label='Predicted Future')\n",
    "    plt.legend(loc='upper left')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x, y in train_data_multi.take(1):\n",
    "    multi_step_plot(x[0], y[0], np.array([0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the task here is a bit more complicated than the previous task, **the model now consists of two LSTM layers**. \n",
    "\n",
    "`multi_step_model` will have\n",
    "- LSTM with 32 units as input\n",
    "- LSTM with 16 units\n",
    "- Dense layer with 3 units\n",
    "\n",
    "Note that the last Dense layer has 3 nodes in order to predict 3 target values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_step_model = tf.keras.models.Sequential([\n",
    "\n",
    "\n",
    "])\n",
    "\n",
    "multi_step_model.compile(optimizer=tf.keras.optimizers.RMSprop(clipvalue=1.0), loss='mae')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EVALUATION_INTERVAL = 200\n",
    "EPOCHS = 10\n",
    "\n",
    "multi_step_history = multi_step_model.fit(train_data_multi, epochs=EPOCHS,\n",
    "                                          steps_per_epoch=EVALUATION_INTERVAL,\n",
    "                                          validation_data=val_data_multi,\n",
    "                                          validation_steps=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_train_history(multi_step_history, 'Multi-Step Training and validation loss')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predict a multi-step future\n",
    "Let's now have a look at how well your network has learnt to predict the future. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x, y in val_data_multi.take(3):\n",
    "    multi_step_plot(x[0], y[0], multi_step_model.predict(x)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can experiment with various model configurations and select best one. Your options are as follows:\n",
    "\n",
    "- Increasae/decrease the context length\n",
    "- Change the number of labels the model will predict\n",
    "- Change the number of parameters of LSTM\n",
    "- Change features to include in learning"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
