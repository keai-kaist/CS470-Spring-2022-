{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS470 Introduction to Artificial Intelligence\n",
    "## Deep Learning Practice \n",
    "#### TA. Yechan Hwang\n",
    "---\n",
    "\n",
    "### Agenda for this practice\n",
    "#### 1. Shakespeare dataset\n",
    "#### 2. GRU Model\n",
    "#### 3. Generating texts\n",
    "---\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6-1. Text generation with an RNN \n",
    "In this practice, we will learn how to generate text using a character-based RNN. We will train a model when given a sequence of characters from this data, that predicts the next character in the sequence. For example, when given the characters 'togethe', trained model will predict 'r' as a next character. Longer sequences of text can be generated by calling the model repeatedly. \n",
    "\n",
    "We will practice with a dataset of **Shakespeare's writing** (from Andrej Karpathy's [The Unreasonable Effectiveness of Recurrent Neural Networks](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)). Our dataset has the format of the screenplay.\n",
    "\n",
    "https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt\n",
    "\n",
    "And before starting the practice, we will upload checkpoint file first which will be used later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers.experimental import preprocessing\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download the Shakespeare dataset\n",
    "Run the following lines to download data for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/root/.keras/datasets/shakespeare.txt\n"
     ]
    }
   ],
   "source": [
    "path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')\n",
    "print(path_to_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read the data\n",
    "First, let's take a look at the length of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of text: 1115394 characters\n"
     ]
    }
   ],
   "source": [
    "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
    "\n",
    "print ('Length of text: {} characters'.format(len(text)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here *length of text* is the number of characters in it. We have more than one million characters.<br/>\n",
    "Also, we can check the first 250 characters in training text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(text[:250])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sample prediction\n",
    "\n",
    "The following is sample output when the model in this practice trained for 30 epochs, and started with the character 'Q'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<pre>\n",
    "QUEENE:\n",
    "I had thought thou hadst a Roman; for the oracle,\n",
    "Thus by All bids the man against the word,\n",
    "Which are so weak of care, by old care done;\n",
    "Your children were in your holy love,\n",
    "And the precipitation through the bleeding throne.\n",
    "\n",
    "BISHOP OF ELY:\n",
    "Marry, and will, my lord, to weep in such a one were prettiest;\n",
    "Yet now I was adopted heir\n",
    "Of the world's lamentable day,\n",
    "To watch the next way with his father with his face?\n",
    "\n",
    "ESCALUS:\n",
    "The cause why then we are all resolved more sons.\n",
    "\n",
    "VOLUMNIA:\n",
    "O, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, it is no sin it should be dead,\n",
    "And love and pale as any will to that word.\n",
    "\n",
    "QUEEN ELIZABETH:\n",
    "But how long have I heard the soul for this world,\n",
    "And show his hands of life be proved to stand.\n",
    "\n",
    "PETRUCHIO:\n",
    "I say he look'd on, if I must be content\n",
    "To stay him from the fatal of our country's bliss.\n",
    "His lordship pluck'd from this sentence then for prey,\n",
    "And then let us twain, being the moon,\n",
    "were she such a case as fills m\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "While most of the sentences are grammatically correct, they do not make sense. But the model seems to have learned some attributes.\n",
    "\n",
    "- Before the training, the model can't know the style of the training data. \n",
    "- But after training, the structure of the output resembles a playâ€”blocks of text generally begin with a speaker name, in all capital letters similar to the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And how many unique characters are there? Let's check it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65 unique characters\n"
     ]
    }
   ],
   "source": [
    "vocab = sorted(set(text))\n",
    "print ('{} unique characters'.format(len(vocab)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\n', ' ', '!', '$', '&', \"'\", ',', '-', '.', '3', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n"
     ]
    }
   ],
   "source": [
    "print(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are some special symbols and characters (including lowercase and uppercase letters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vectorize the text\n",
    "Before training, we need to **map all the characters in the dataset to a numerical representation**. \n",
    "\n",
    "We will create two lookup tables: \n",
    "- one for mapping **characters to numbers** (char2idx)\n",
    "- another for **numbers to characters** (idx2char)\n",
    "\n",
    "And we can vectorize the text data using char2idx."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a mapping from unique characters to indices\n",
    "char2idx = {}\n",
    "for i in range(len(vocab)):\n",
    "    char2idx[vocab[i]]=i\n",
    "\n",
    "idx2char = np.array(vocab)\n",
    "\n",
    "# 1D integer vector for all characters in the text data\n",
    "text_as_int = np.array([char2idx[c] for c in text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'\\n': 0, ' ': 1, '!': 2, '$': 3, '&': 4, \"'\": 5, ',': 6, '-': 7, '.': 8, '3': 9, ':': 10, ';': 11, '?': 12, 'A': 13, 'B': 14, 'C': 15, 'D': 16, 'E': 17, 'F': 18, 'G': 19, 'H': 20, 'I': 21, 'J': 22, 'K': 23, 'L': 24, 'M': 25, 'N': 26, 'O': 27, 'P': 28, 'Q': 29, 'R': 30, 'S': 31, 'T': 32, 'U': 33, 'V': 34, 'W': 35, 'X': 36, 'Y': 37, 'Z': 38, 'a': 39, 'b': 40, 'c': 41, 'd': 42, 'e': 43, 'f': 44, 'g': 45, 'h': 46, 'i': 47, 'j': 48, 'k': 49, 'l': 50, 'm': 51, 'n': 52, 'o': 53, 'p': 54, 'q': 55, 'r': 56, 's': 57, 't': 58, 'u': 59, 'v': 60, 'w': 61, 'x': 62, 'y': 63, 'z': 64}\n"
     ]
    }
   ],
   "source": [
    "print(char2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\n' ' ' '!' '$' '&' \"'\" ',' '-' '.' '3' ':' ';' '?' 'A' 'B' 'C' 'D' 'E'\n",
      " 'F' 'G' 'H' 'I' 'J' 'K' 'L' 'M' 'N' 'O' 'P' 'Q' 'R' 'S' 'T' 'U' 'V' 'W'\n",
      " 'X' 'Y' 'Z' 'a' 'b' 'c' 'd' 'e' 'f' 'g' 'h' 'i' 'j' 'k' 'l' 'm' 'n' 'o'\n",
      " 'p' 'q' 'r' 's' 't' 'u' 'v' 'w' 'x' 'y' 'z']\n"
     ]
    }
   ],
   "source": [
    "print(idx2char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18 47 56 ... 45  8  0]\n",
      "(1115394,)\n"
     ]
    }
   ],
   "source": [
    "print(text_as_int)\n",
    "print(text_as_int.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/><br/>\n",
    "Also let's check how the first 13 characters from the dataset text are mapped to integers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen ---- characters mapped to int ---- > [18 47 56 57 58  1 15 47 58 47 64 43 52]\n"
     ]
    }
   ],
   "source": [
    "first_text_as_int = text_as_int[:13]\n",
    "\n",
    "print ('{} ---- characters mapped to int ---- > {}'.format(text[:13], first_text_as_int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18 47 56 57 58  1 15 47 58 47 64 43 52] ---- ints mapped to characters ---- > ['F' 'i' 'r' 's' 't' ' ' 'C' 'i' 't' 'i' 'z' 'e' 'n']\n"
     ]
    }
   ],
   "source": [
    "print ('{} ---- ints mapped to characters ---- > {}'.format(first_text_as_int, idx2char[first_text_as_int]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The prediction task\n",
    "Our goal is to predict **the most probable following character** when given a character or a sequence of characters. Therefore, the **input to the model will be a sequence of characters** and the model will learn to **predict the output : the following character at each time step**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create training examples and targets\n",
    "For now, we will divide the text data into training sequences. Each input sequence will contain `seq_length` characters from the text. For each input sequence, the corresponding targets contain the same length of text, but shifted one character to the right.\n",
    "\n",
    "Therefore, the steps for making training input/target are as follows :\n",
    "1. Break all the text into chunks of `seq_length+1`.\n",
    "2. Input data is whole characters except the last character.\n",
    "3. Target data is whole characters except the first character.\n",
    "\n",
    "For example, let's say that `seq_length` is 4 and our training text is \"HELLO\".\n",
    "In this example, **the input sequence would be \"HELL\", and the target sequence \"ELLO\"**.\n",
    "\n",
    "<img src=\"images/teacher_forcing.png\" alt=\"Drawing\" style=\"width: 700px;\"/>\n",
    "\n",
    "To do this, first use the [`tf.data.Dataset.from_tensor_slices`](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/data/Dataset#from_tensor_slices) function to convert the text vector into a stream of character indices.\n",
    "- `tf.data.Dataset.from_tensor_slices`: Creates a Dataset whose elements are slices of the given tensors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18 : F\n",
      "47 : i\n",
      "56 : r\n",
      "57 : s\n",
      "58 : t\n",
      "1 :  \n",
      "15 : C\n",
      "47 : i\n",
      "58 : t\n",
      "47 : i\n",
      "64 : z\n",
      "43 : e\n",
      "52 : n\n",
      "10 : :\n",
      "0 : \n",
      "\n",
      "14 : B\n",
      "43 : e\n",
      "44 : f\n",
      "53 : o\n",
      "56 : r\n"
     ]
    }
   ],
   "source": [
    "# Make char dataset (in the form of integer)\n",
    "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
    "\n",
    "for i in char_dataset.take(20):\n",
    "    print(str(int(i))+\" : \"+str(idx2char[int(i)]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 11043 sequences of length 101\n",
      "\n",
      "101\n",
      "[18 47 56 57 58  1 15 47 58 47 64 43 52 10  0 14 43 44 53 56 43  1 61 43\n",
      "  1 54 56 53 41 43 43 42  1 39 52 63  1 44 59 56 58 46 43 56  6  1 46 43\n",
      " 39 56  1 51 43  1 57 54 43 39 49  8  0  0 13 50 50 10  0 31 54 43 39 49\n",
      "  6  1 57 54 43 39 49  8  0  0 18 47 56 57 58  1 15 47 58 47 64 43 52 10\n",
      "  0 37 53 59  1]\n",
      "'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n",
      "\n",
      "101\n",
      "[39 56 43  1 39 50 50  1 56 43 57 53 50 60 43 42  1 56 39 58 46 43 56  1\n",
      " 58 53  1 42 47 43  1 58 46 39 52  1 58 53  1 44 39 51 47 57 46 12  0  0\n",
      " 13 50 50 10  0 30 43 57 53 50 60 43 42  8  1 56 43 57 53 50 60 43 42  8\n",
      "  0  0 18 47 56 57 58  1 15 47 58 47 64 43 52 10  0 18 47 56 57 58  6  1\n",
      " 63 53 59  1 49]\n",
      "'are all resolved rather to die than to famish?\\n\\nAll:\\nResolved. resolved.\\n\\nFirst Citizen:\\nFirst, you k'\n",
      "\n",
      "101\n",
      "[52 53 61  1 15 39 47 59 57  1 25 39 56 41 47 59 57  1 47 57  1 41 46 47\n",
      " 43 44  1 43 52 43 51 63  1 58 53  1 58 46 43  1 54 43 53 54 50 43  8  0\n",
      "  0 13 50 50 10  0 35 43  1 49 52 53 61  5 58  6  1 61 43  1 49 52 53 61\n",
      "  5 58  8  0  0 18 47 56 57 58  1 15 47 58 47 64 43 52 10  0 24 43 58  1\n",
      " 59 57  1 49 47]\n",
      "\"now Caius Marcius is chief enemy to the people.\\n\\nAll:\\nWe know't, we know't.\\n\\nFirst Citizen:\\nLet us ki\"\n",
      "\n",
      "101\n",
      "[50 50  1 46 47 51  6  1 39 52 42  1 61 43  5 50 50  1 46 39 60 43  1 41\n",
      " 53 56 52  1 39 58  1 53 59 56  1 53 61 52  1 54 56 47 41 43  8  0 21 57\n",
      "  5 58  1 39  1 60 43 56 42 47 41 58 12  0  0 13 50 50 10  0 26 53  1 51\n",
      " 53 56 43  1 58 39 50 49 47 52 45  1 53 52  5 58 11  1 50 43 58  1 47 58\n",
      "  1 40 43  1 42]\n",
      "\"ll him, and we'll have corn at our own price.\\nIs't a verdict?\\n\\nAll:\\nNo more talking on't; let it be d\"\n",
      "\n",
      "101\n",
      "[53 52 43 10  1 39 61 39 63  6  1 39 61 39 63  2  0  0 31 43 41 53 52 42\n",
      "  1 15 47 58 47 64 43 52 10  0 27 52 43  1 61 53 56 42  6  1 45 53 53 42\n",
      "  1 41 47 58 47 64 43 52 57  8  0  0 18 47 56 57 58  1 15 47 58 47 64 43\n",
      " 52 10  0 35 43  1 39 56 43  1 39 41 41 53 59 52 58 43 42  1 54 53 53 56\n",
      "  1 41 47 58 47]\n",
      "'one: away, away!\\n\\nSecond Citizen:\\nOne word, good citizens.\\n\\nFirst Citizen:\\nWe are accounted poor citi'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Make sequences with sequence length +1\n",
    "seq_length = 100\n",
    "sequences = char_dataset.batch(seq_length+1, drop_remainder=True)\n",
    "\n",
    "print(\"There are \"+str(len(sequences))+\" sequences of length \"+str(seq_length+1))\n",
    "print()\n",
    "\n",
    "for item in sequences.take(5):\n",
    "    print(len(idx2char[item.numpy()]))\n",
    "    print(item.numpy())\n",
    "    print(repr(''.join(idx2char[item.numpy()])))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/><br/>\n",
    "\n",
    "Now we have to convert above text into input data and target data. Note that target data must be shifted one character to the right.\n",
    "\n",
    "(Input data is whole characters except the last character, Target data is whole characters except the first character.)\n",
    "\n",
    "To do this, we will use `tf.data.Dataset.map`. When we give some function to `tf.data.Dataset.map` as a parameter, it will apply the function to all elements and then return them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 4, 5]\n",
      "[2, 3, 4, 5, 6]\n"
     ]
    }
   ],
   "source": [
    "def plus_1(x):\n",
    "    return x+1\n",
    "    \n",
    "temp_dataset = tf.data.Dataset.range(1, 6)  # ==> [ 1, 2, 3, 4, 5 ]\n",
    "print(list(temp_dataset.as_numpy_iterator()))\n",
    "temp_dataset = temp_dataset.map(plus_1)\n",
    "print(list(temp_dataset.as_numpy_iterator()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/><br/>\n",
    "Here, let's define a function 'construct_input_target' which returns input data and target data as explained above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_input_target(chunk):\n",
    "    input_text = chunk[:-1]\n",
    "    target_text = chunk[1:]\n",
    "    return input_text, target_text\n",
    "\n",
    "dataset = sequences.map(construct_input_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data:  'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou'\n",
      "Target data: 'irst Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n",
      "\n",
      "Input data:  'are all resolved rather to die than to famish?\\n\\nAll:\\nResolved. resolved.\\n\\nFirst Citizen:\\nFirst, you '\n",
      "Target data: 're all resolved rather to die than to famish?\\n\\nAll:\\nResolved. resolved.\\n\\nFirst Citizen:\\nFirst, you k'\n",
      "\n",
      "Input data:  \"now Caius Marcius is chief enemy to the people.\\n\\nAll:\\nWe know't, we know't.\\n\\nFirst Citizen:\\nLet us k\"\n",
      "Target data: \"ow Caius Marcius is chief enemy to the people.\\n\\nAll:\\nWe know't, we know't.\\n\\nFirst Citizen:\\nLet us ki\"\n",
      "\n",
      "Input data:  \"ll him, and we'll have corn at our own price.\\nIs't a verdict?\\n\\nAll:\\nNo more talking on't; let it be \"\n",
      "Target data: \"l him, and we'll have corn at our own price.\\nIs't a verdict?\\n\\nAll:\\nNo more talking on't; let it be d\"\n",
      "\n",
      "Input data:  'one: away, away!\\n\\nSecond Citizen:\\nOne word, good citizens.\\n\\nFirst Citizen:\\nWe are accounted poor cit'\n",
      "Target data: 'ne: away, away!\\n\\nSecond Citizen:\\nOne word, good citizens.\\n\\nFirst Citizen:\\nWe are accounted poor citi'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for input_example, target_example in dataset.take(5):\n",
    "    print ('Input data: ', repr(''.join(idx2char[input_example.numpy()])))\n",
    "    print ('Target data:', repr(''.join(idx2char[target_example.numpy()])))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>\n",
    "\n",
    "During the training, **each index of these vectors are processed as one time step**. For the input at time step 0, the model receives the index for \"F\" and tries to predict the index for \"i\" as the next character. At the next timestep, it does the same thing but the **RNN considers the previous step context in addition to the current input character**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step    0\n",
      "  input: 53 ('o')\n",
      "  expected output: 52 ('n')\n",
      "\n",
      "Step    1\n",
      "  input: 52 ('n')\n",
      "  expected output: 43 ('e')\n",
      "\n",
      "Step    2\n",
      "  input: 43 ('e')\n",
      "  expected output: 10 (':')\n",
      "\n",
      "Step    3\n",
      "  input: 10 (':')\n",
      "  expected output: 1 (' ')\n",
      "\n",
      "Step    4\n",
      "  input: 1 (' ')\n",
      "  expected output: 39 ('a')\n",
      "\n",
      "Step    5\n",
      "  input: 39 ('a')\n",
      "  expected output: 61 ('w')\n",
      "\n",
      "Step    6\n",
      "  input: 61 ('w')\n",
      "  expected output: 39 ('a')\n",
      "\n",
      "Step    7\n",
      "  input: 39 ('a')\n",
      "  expected output: 63 ('y')\n",
      "\n",
      "Step    8\n",
      "  input: 63 ('y')\n",
      "  expected output: 6 (',')\n",
      "\n",
      "Step    9\n",
      "  input: 6 (',')\n",
      "  expected output: 1 (' ')\n",
      "\n",
      "Step   10\n",
      "  input: 1 (' ')\n",
      "  expected output: 39 ('a')\n",
      "\n",
      "Step   11\n",
      "  input: 39 ('a')\n",
      "  expected output: 61 ('w')\n",
      "\n",
      "Step   12\n",
      "  input: 61 ('w')\n",
      "  expected output: 39 ('a')\n",
      "\n",
      "Step   13\n",
      "  input: 39 ('a')\n",
      "  expected output: 63 ('y')\n",
      "\n",
      "Step   14\n",
      "  input: 63 ('y')\n",
      "  expected output: 2 ('!')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, (input_idx, target_idx) in enumerate(zip(input_example[:15], target_example[:15])):\n",
    "    print(\"Step {:4d}\".format(i))\n",
    "    print(\"  input: {} ({:s})\".format(input_idx, repr(idx2char[input_idx])))\n",
    "    print(\"  expected output: {} ({:s})\".format(target_idx, repr(idx2char[target_idx])))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create training batches\n",
    "We used `tf.data` to split the text into manageable sequences. But before feeding this data into the model, we need to **shuffle the data and pack it into batches**.\n",
    "\n",
    "[`tf.data.Dataset.shuffle`](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/data/Dataset#shuffle)(buffer_size, seed=None, reshuffle_each_iteration=None) : Randomly shuffles the elements of this dataset.\n",
    "\n",
    "[`tf.data.Dataset.batch`](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/data/Dataset#batch)(batch_size, drop_remainder=False) : Combines consecutive elements of this dataset into batches.\n",
    "\n",
    "Note that `tf.data.Dataset.shuffle` **doesn't shuffle characters within each sentence**, but the sentences in dataset will be shuffled by sentences.\n",
    "\n",
    "<img src=\"images/shuffle1.png\" alt=\"Drawing\" style=\"width: 600px;\"/>\n",
    "\n",
    "<br/>\n",
    "<br/>\n",
    "\n",
    "<img src=\"images/shuffle2.png\" alt=\"Drawing\" style=\"width: 600px;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset shapes: ((64, 100), (64, 100)), types: (tf.int64, tf.int64)>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Batch size\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# Buffer size to shuffle the dataset\n",
    "# (TF data is designed to work with possibly infinite sequences,\n",
    "# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
    "# it maintains a buffer in which it shuffles elements).\n",
    "BUFFER_SIZE = 10000\n",
    "\n",
    "# Shuffle the data and create batches (1 data = (100, 100) ==> 0:99, 1:100)\n",
    "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/><br/>\n",
    "\n",
    "We can see that each batch has 64 input sentences (each has 100 characters) and 64 target sentences (each has 100 characters). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[ 1 52 53 ... 43 52 11]\n",
      " [ 7 45 53 ...  1 44 53]\n",
      " [39 52 52 ... 53 60 43]\n",
      " ...\n",
      " [43  1 51 ... 19 14 30]\n",
      " [10  0 26 ... 43  1 63]\n",
      " [ 8  0  0 ...  1 51 39]], shape=(64, 100), dtype=int64) tf.Tensor(\n",
      "[[52 53  1 ... 52 11  0]\n",
      " [45 53 60 ... 44 53 56]\n",
      " [52 52 53 ... 60 43  1]\n",
      " ...\n",
      " [ 1 51 63 ... 14 30 27]\n",
      " [ 0 26 39 ...  1 63 53]\n",
      " [ 0  0 29 ... 51 39 49]], shape=(64, 100), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "for input_example_batch, target_example_batch in dataset.take(1):\n",
    "    print(input_example_batch, target_example_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/><br/>\n",
    "\n",
    "Also we can see that all the target sentences are shifted one character to the right ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[ 1 44 39 58 46 43 56  5 57  1 46 53 59 57 43  6  0 35 46 53  1 45 39 60\n",
      " 43  1 46 47 57  1 40 50 53 53 42  1 58 53  1 50 47 51 43  1 58 46 43  1\n",
      " 57 58 53 52 43 57  1 58 53 45 43 58 46 43 56  6  0 13 52 42  1 57 43 58\n",
      "  1 59 54  1 24 39 52 41 39 57 58 43 56  8  1 35 46 63  6  1 58 56 53 61\n",
      "  5 57 58  1], shape=(100,), dtype=int64)\n",
      "tf.Tensor(\n",
      "[44 39 58 46 43 56  5 57  1 46 53 59 57 43  6  0 35 46 53  1 45 39 60 43\n",
      "  1 46 47 57  1 40 50 53 53 42  1 58 53  1 50 47 51 43  1 58 46 43  1 57\n",
      " 58 53 52 43 57  1 58 53 45 43 58 46 43 56  6  0 13 52 42  1 57 43 58  1\n",
      " 59 54  1 24 39 52 41 39 57 58 43 56  8  1 35 46 63  6  1 58 56 53 61  5\n",
      " 57 58  1 58], shape=(100,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "for input_example_batch, target_example_batch in dataset.take(1):\n",
    "    print(input_example_batch[0])\n",
    "    print(target_example_batch[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build The GRU Model\n",
    "We will use `tf.keras.Sequential` to define the model. And for the model in this practice, three layers will be used:\n",
    "\n",
    "- `tf.keras.layers.Embedding`: The input layer. A trainable lookup table that will map the numbers of each character to a vector with embedding_dim dimensions;\n",
    "- [`tf.keras.layers.GRU`](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/layers/GRU): A type of RNN with size units=rnn_units (You can also use a LSTM layer here.)\n",
    "- `tf.keras.layers.Dense`: The output layer, with vocab_size outputs.\n",
    "\n",
    "<img src=https://miro.medium.com/max/2400/1*dhq14CzJijlqjf7IlDB0uw.png>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### About the GRU\n",
    "\n",
    "GRU is a variation of LSTM. GRU has some different attributes compared to vanilla LSTM.\n",
    "\n",
    "- The two state vectors $c_t$ and $h_t$ in the LSTM Cell are merged into one vector $h_t$.\n",
    "- There is only one gate controller $z_t$ that controls all input gates.\n",
    "- There is no output gate and the state vector $h_t$ is the output of GRU.\n",
    "\n",
    "You can see details about the GRU at the this [link](https://arxiv.org/abs/1406.1078).\n",
    "In this practice, we will use GRU since its operation is faster than LSTM and it has fewer parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Length of the vocabulary in chars\n",
    "vocab_size = 65 # len(vocab)\n",
    "\n",
    "# The embedding dimension\n",
    "embedding_dim = 256\n",
    "\n",
    "# Number of RNN units\n",
    "rnn_units = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import Sequential \n",
    "from tensorflow.keras.layers import Embedding, GRU, Dense \n",
    "\n",
    "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
    "    model = Sequential([\n",
    "        Embedding(vocab_size, embedding_dim,\n",
    "                  batch_input_shape=[batch_size, None]\n",
    "        ),\n",
    "        \n",
    "        GRU(rnn_units, # Positive integer, dimensionality of the output space.\n",
    "            return_sequences=True, # Whether to return just last output only or the full sequence.\n",
    "            stateful=True,  #If True, the last state for each sample at index i in a batch will be used as initial state for the sample of index i in the following batch.\n",
    "            recurrent_initializer='glorot_uniform'\n",
    "        ),\n",
    "        \n",
    "        Dense(vocab_size)\n",
    "    ])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (64, None, 256)           16640     \n",
      "_________________________________________________________________\n",
      "gru (GRU)                    (64, None, 1024)          3938304   \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (64, None, 65)            66625     \n",
      "=================================================================\n",
      "Total params: 4,021,569\n",
      "Trainable params: 4,021,569\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = build_model(\n",
    "    vocab_size = len(vocab),\n",
    "    embedding_dim=embedding_dim,\n",
    "    rnn_units=rnn_units,\n",
    "    batch_size=BATCH_SIZE)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each character the model looks up the embedding, runs the GRU one timestep with the embedding as input, and applies the dense layer to generate logits predicting the log-likelihood of the next character:\n",
    "\n",
    "<img src=https://www.tensorflow.org/text/tutorials/images/text_generation_training.png>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Try the untrained model\n",
    "Now we will run the untrained model to see how it behaves.\n",
    "\n",
    "First let's check the shape of the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 100) # (batch_size, sequence_length)\n",
      "(64, 100, 65) # (batch_size, sequence_length, vocab_size)\n"
     ]
    }
   ],
   "source": [
    "for input_example_batch, target_example_batch in dataset.take(1):\n",
    "    example_batch_predictions = model(input_example_batch)\n",
    "    print(input_example_batch.shape, \"# (batch_size, sequence_length)\") \n",
    "    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/><br/>\n",
    "\n",
    "Also we can check the model's prediction by probability distribution. For example, we can see the model's prediction for the first sentence's fifth character.\n",
    "\n",
    "(Note that since the current model is not trained yet.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[-5.7021636e-03 -2.8552175e-03  1.9934154e-03 -1.9802842e-03\n",
      " -7.8367200e-03  1.4964208e-03  1.3193447e-02  1.1076080e-02\n",
      " -1.6079998e-02  5.9543708e-03  1.2915297e-03  1.5347846e-02\n",
      "  2.4245273e-02  4.0625734e-04  6.4252410e-04  1.5342148e-02\n",
      " -1.0259328e-02  3.8642762e-04 -1.8401940e-03  1.1804763e-02\n",
      "  1.6406330e-03  2.3928382e-03 -1.4672484e-03 -3.7902817e-03\n",
      "  5.7984591e-03  8.7341527e-05 -1.9185720e-02 -1.1359070e-02\n",
      "  4.1295560e-03  1.8590938e-02 -8.7830592e-03 -1.5000392e-02\n",
      " -1.9850604e-02  1.0131655e-02  2.2139070e-03  3.5425317e-03\n",
      " -9.0296585e-03  1.4619655e-02 -1.3905108e-02  3.5725557e-04\n",
      "  1.3277170e-02 -5.8074114e-03  4.7223978e-03 -1.5757846e-02\n",
      "  9.3864594e-03  9.4863204e-03 -5.8165621e-03  2.6426336e-03\n",
      " -5.0377073e-03 -2.7695575e-03  3.5837949e-03 -1.3579380e-02\n",
      " -1.7965563e-03 -1.2047352e-02 -3.1344555e-03 -3.6765996e-03\n",
      "  5.9711821e-03  1.3788828e-02  1.5896419e-02 -9.6975744e-04\n",
      " -1.1437000e-03  2.3107115e-02 -9.8845093e-03  4.3003848e-03\n",
      " -6.6951304e-03], shape=(65,), dtype=float32)\n",
      "tf.Tensor(\n",
      "[0.01528795 0.01533154 0.01540605 0.01534496 0.01525535 0.0153984\n",
      " 0.01557957 0.01554662 0.01513012 0.0154672  0.01539524 0.01561317\n",
      " 0.01575271 0.01538162 0.01538526 0.01561308 0.01521844 0.01538132\n",
      " 0.01534711 0.01555795 0.01540062 0.01541221 0.01535283 0.01531721\n",
      " 0.01546479 0.01537672 0.0150832  0.01520171 0.015439   0.01566389\n",
      " 0.01524092 0.01514646 0.01507317 0.01553194 0.01540945 0.01542994\n",
      " 0.01523716 0.01560181 0.01516306 0.01538087 0.01558088 0.01528634\n",
      " 0.01544815 0.01513499 0.01552037 0.01552192 0.0152862  0.01541606\n",
      " 0.01529811 0.01533285 0.01543058 0.015168   0.01534778 0.01519125\n",
      " 0.01532726 0.01531895 0.01546746 0.01558885 0.01562174 0.01536047\n",
      " 0.0153578  0.01573479 0.01522414 0.01544164 0.01527278], shape=(65,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(example_batch_predictions[0][5])\n",
    "print(tf.nn.softmax(example_batch_predictions[0][5]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Free input length\n",
    "\n",
    "In our practice, the sequence length of the input in oyr dataset is 100 but the model can be run on inputs of any length, which is an advantage of the recurrent neural network which can handle inputs of variable length."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get actual predictions from the model, we need to sample from the output distribution to get actual character indices. This distribution is defined by the logits over the character vocabulary.\n",
    "\n",
    "Try it for the first example in the batch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[53 50 53 52 45  1 39 61 46 47 50 43  1 58 46 43  1 58 56 39 47 58 53 56\n",
      "  5 57  1 50 47 44 43  8  0 35 56 39 58 46  1 51 39 49 43 57  1 46 47 51\n",
      "  1 42 43 39 44 10  1 57 54 43 39 49  1 58 46 53 59  6  1 26 53 56 58 46\n",
      " 59 51 40 43 56 50 39 52 42  8  0  0 26 27 30 32 20 33 25 14 17 30 24 13\n",
      " 26 16 10  0], shape=(100,), dtype=int64)\n",
      "\n",
      "tf.Tensor(\n",
      "[[58]\n",
      " [61]\n",
      " [30]\n",
      " [31]\n",
      " [ 0]\n",
      " [51]\n",
      " [44]\n",
      " [52]\n",
      " [22]\n",
      " [41]\n",
      " [43]\n",
      " [44]\n",
      " [ 4]\n",
      " [60]\n",
      " [48]\n",
      " [32]\n",
      " [ 2]\n",
      " [37]\n",
      " [27]\n",
      " [53]\n",
      " [51]\n",
      " [52]\n",
      " [12]\n",
      " [61]\n",
      " [20]\n",
      " [62]\n",
      " [20]\n",
      " [11]\n",
      " [29]\n",
      " [15]\n",
      " [37]\n",
      " [62]\n",
      " [35]\n",
      " [53]\n",
      " [12]\n",
      " [60]\n",
      " [35]\n",
      " [43]\n",
      " [24]\n",
      " [24]\n",
      " [35]\n",
      " [22]\n",
      " [ 1]\n",
      " [33]\n",
      " [25]\n",
      " [29]\n",
      " [38]\n",
      " [37]\n",
      " [ 8]\n",
      " [32]\n",
      " [17]\n",
      " [57]\n",
      " [21]\n",
      " [62]\n",
      " [59]\n",
      " [28]\n",
      " [26]\n",
      " [63]\n",
      " [ 1]\n",
      " [23]\n",
      " [60]\n",
      " [42]\n",
      " [21]\n",
      " [ 2]\n",
      " [23]\n",
      " [36]\n",
      " [15]\n",
      " [11]\n",
      " [43]\n",
      " [24]\n",
      " [36]\n",
      " [55]\n",
      " [18]\n",
      " [56]\n",
      " [27]\n",
      " [16]\n",
      " [55]\n",
      " [12]\n",
      " [51]\n",
      " [59]\n",
      " [58]\n",
      " [35]\n",
      " [34]\n",
      " [47]\n",
      " [15]\n",
      " [16]\n",
      " [24]\n",
      " [31]\n",
      " [42]\n",
      " [52]\n",
      " [12]\n",
      " [56]\n",
      " [59]\n",
      " [40]\n",
      " [21]\n",
      " [22]\n",
      " [49]\n",
      " [50]\n",
      " [36]\n",
      " [21]], shape=(100, 1), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "# num_samples : determines how many characters to sample at each iteration\n",
    "\n",
    "print(input_example_batch[0])\n",
    "print()\n",
    "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
    "print(sampled_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This gives us a prediction for the next character index at each timestep.\n",
    "\n",
    "Now in order to check the predicted sentence of our untrained model, we will squeeze the `sampled_indices` and convert them into characters.\n",
    "- [`tf.squeeze`](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/squeeze): Removes dimensions of size 1 from the shape of a tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100,)\n",
      "[58 61 30 31  0 51 44 52 22 41 43 44  4 60 48 32  2 37 27 53 51 52 12 61\n",
      " 20 62 20 11 29 15 37 62 35 53 12 60 35 43 24 24 35 22  1 33 25 29 38 37\n",
      "  8 32 17 57 21 62 59 28 26 63  1 23 60 42 21  2 23 36 15 11 43 24 36 55\n",
      " 18 56 27 16 55 12 51 59 58 35 34 47 15 16 24 31 42 52 12 56 59 40 21 22\n",
      " 49 50 36 21]\n"
     ]
    }
   ],
   "source": [
    "sampled_indices = tf.squeeze(sampled_indices,axis=-1).numpy()\n",
    "\n",
    "print(sampled_indices.shape)\n",
    "print(sampled_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After sqeezing the `sampled_indices`, we got 1D vector that contains indicies of predicted characters.\n",
    "\n",
    "Let's decode this vector to see the text predicted by this untrained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: \n",
      " \"olong awhile the traitor's life.\\nWrath makes him deaf: speak thou, Northumberland.\\n\\nNORTHUMBERLAND:\\n\"\n",
      "\n",
      "Next Char Predictions: \n",
      " 'twRS\\nmfnJcef&vjT!YOomn?wHxH;QCYxWo?vWeLLWJ UMQZY.TEsIxuPNy KvdI!KXC;eLXqFrODq?mutWViCDLSdn?rubIJklXI'\n"
     ]
    }
   ],
   "source": [
    "print(\"Input: \\n\", repr(\"\".join(idx2char[input_example_batch[0]])))\n",
    "print()\n",
    "print(\"Next Char Predictions: \\n\", repr(\"\".join(idx2char[sampled_indices ])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>\n",
    "\n",
    "Since our model is not trained yet, it seems to just predict next character randomly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train the model\n",
    "At this point the problem can be treated as a standard classification problem. **Given the previous RNN state and the input character at each time step, our model must predict the next character.**\n",
    "\n",
    "#### Compile the model\n",
    "We will use `tf.keras.losses.sparse_categorical_crossentropy` loss function since it works well for classification problem.\n",
    "\n",
    "Since our model returns logits, we need to set the `from_logits` flag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(labels, logits):\n",
    "    return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
    "\n",
    "model.compile(optimizer='adam', \n",
    "              loss=loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Configure checkpoints\n",
    "Use a [`tf.keras.callbacks.ModelCheckpoint`](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/callbacks/ModelCheckpoint) to ensure that checkpoints are saved during training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory where the checkpoints will be saved\n",
    "checkpoint_dir = './training_checkpoints'\n",
    "# Name of the checkpoint files\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
    "\n",
    "checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_prefix,\n",
    "    save_weights_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(dataset, \n",
    "                    epochs=15, \n",
    "                    callbacks=[checkpoint_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/training_result.png\" alt=\"Drawing\" style=\"width: 600px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate text\n",
    "We will restore the latest checkpoint. Then, to keep this prediction step simple, we will use 1 for batch size.\n",
    "\n",
    "(Note that in order to run the model with a different `batch_size`, we need to rebuild the model with different batch size and restore the weights from the checkpoint.)\n",
    "\n",
    "By the codes below, we can check the path that contains weights for the lastest model and load it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rebuild the model by changing batch size (=1) to predict new text\n",
    "model = build_model(vocab_size, embedding_dim, rnn_units, batch_size=1)\n",
    "\n",
    "# Load the weight of the model we trained \n",
    "# model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
    "model.load_weights('./saved_ckpt/ckpt_15')\n",
    "\n",
    "# Change the batch size from 64 to 1\n",
    "model.build(tf.TensorShape([1, None]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (1, None, 256)            16640     \n",
      "_________________________________________________________________\n",
      "gru_1 (GRU)                  (1, None, 1024)           3938304   \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (1, None, 65)             66625     \n",
      "=================================================================\n",
      "Total params: 4,021,569\n",
      "Trainable params: 4,021,569\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The prediction loop\n",
    "\n",
    "1. Start by choosing a **start string** and initialize the RNN hidden state for the first iteration.\n",
    "2. Set the number of characters to generate.\n",
    "3. Get the **prediction distribution of the next character using the start string and hidden state**.\n",
    "4. Sample an index of the predicted character using a multinomial distribution of the first iteration. \n",
    "5. Use this predicted character as our next input to the model.\n",
    "6. Repeat step 3-5 until we get the number of characters we set.\n",
    "\n",
    "**Note that the RNN hidden state returned by the model is fed back into the model and hidden state will become more complex as the prediction loop repeats.**\n",
    "In other words, after predicting the a word, the modified RNN states are again fed back into the model, which is how the model learns as it gets more context from the previously predicted words.\n",
    "\n",
    "\n",
    "![To generate text the model's output is fed back to the input](https://www.tensorflow.org/text/tutorials/images/text_generation_sampling.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, start_string, num_generate,temperature):\n",
    "    # Evaluation step (generating text using the learned model)\n",
    "\n",
    "    # Converting our start string to numbers (vectorizing)\n",
    "    input_eval = [char2idx[s] for s in start_string]\n",
    "    input_eval = tf.expand_dims(input_eval, 0)\n",
    "\n",
    "    # Empty string to store our results\n",
    "    text_generated = []\n",
    "\n",
    "    # Here batch size == 1\n",
    "    model.reset_states()\n",
    "    for i in range(num_generate):\n",
    "        predictions = model(input_eval)\n",
    "        # remove the batch dimension\n",
    "        predictions = tf.squeeze(predictions, 0)\n",
    "\n",
    "        # using a categorical distribution to predict the word returned by the model\n",
    "        predictions = predictions / temperature\n",
    "        predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
    "\n",
    "        # We pass the predicted word as the next input to the model\n",
    "        # along with the previous hidden state\n",
    "        input_eval = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "        text_generated.append(idx2char[predicted_id])\n",
    "\n",
    "    return (start_string + ''.join(text_generated))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I will go home and sleep,\n",
      "Turn troops of quality: and, in good correlt,\n",
      "That is a serina--commander, hen my reediness\n",
      "He has deap to conceal intelcession of it.\n",
      "\n",
      "MENENIUS:\n",
      "Beshrew your grace, unitsul?\n",
      "O, ill be with yoke more joint\n",
      "You are beturn and findily and his troth,\n",
      "Which often so hot revenge's young man do;\n",
      "For thou hast arteen balladement than thou lack's unck,\n",
      "The days show wear I seem\n",
      "Savest the victor's mouth, or hadments must deward:\n",
      "I would say so hold by curstabeth.\n",
      "\n",
      "VOLUMNIA:\n",
      "I kill'd! I have left their\n"
     ]
    }
   ],
   "source": [
    "# Low temperatures results in more predictable text.\n",
    "# Higher temperatures results in more surprising text.\n",
    "# Experiment to find the best setting.\n",
    "\n",
    "print(generate_text(model, start_string=\"I will go home and sleep\", num_generate = 500, temperature=1.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/><br/>\n",
    "\n",
    "Looking at the generated text, you'll see the model knows when to capitalize and make paragraphs, and it imitates a Shakespeare-like writing vocabulary.\n",
    "\n",
    "<br/>\n",
    "The easiest thing you can do to improve the results is to train it for longer (e.g. try EPOCHS=30).\n",
    "\n",
    "You can also experiment with a different start string, or try adding another RNN layer to improve the model's accuracy, or adjusting the temperature parameter to generate more or less random predictions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
