{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS470 Introduction to Artificial Intelligence\n",
    "## Deep Learning Practice \n",
    "#### TA. Yechan Hwang\n",
    "---\n",
    "\n",
    "### Agenda for this practice\n",
    "#### 1. Shakespeare dataset\n",
    "#### 2. GRU Model\n",
    "#### 3. Generating texts\n",
    "---\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6-1. Text generation with an RNN \n",
    "In this practice, we will learn how to generate text using a character-based RNN. We will train a model when given a sequence of characters from this data, that predicts the next character in the sequence. For example, when given the characters 'togethe', trained model will predict 'r' as a next character. Longer sequences of text can be generated by calling the model repeatedly. \n",
    "\n",
    "We will practice with a dataset of **Shakespeare's writing** (from Andrej Karpathy's [The Unreasonable Effectiveness of Recurrent Neural Networks](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)). Our dataset has the format of the screenplay.\n",
    "\n",
    "https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt\n",
    "\n",
    "And before starting the practice, we will upload checkpoint file first which will be used later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers.experimental import preprocessing\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download the Shakespeare dataset\n",
    "Run the following lines to download data for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')\n",
    "print(path_to_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read the data\n",
    "First, let's take a look at the length of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
    "\n",
    "print ('Length of text: {} characters'.format(len(text)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here *length of text* is the number of characters in it. We have more than one million characters.<br/>\n",
    "Also, we can check the first 250 characters in training text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(text[:250])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sample prediction\n",
    "\n",
    "The following is sample output when the model in this practice trained for 30 epochs, and started with the character 'Q'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<pre>\n",
    "QUEENE:\n",
    "I had thought thou hadst a Roman; for the oracle,\n",
    "Thus by All bids the man against the word,\n",
    "Which are so weak of care, by old care done;\n",
    "Your children were in your holy love,\n",
    "And the precipitation through the bleeding throne.\n",
    "\n",
    "BISHOP OF ELY:\n",
    "Marry, and will, my lord, to weep in such a one were prettiest;\n",
    "Yet now I was adopted heir\n",
    "Of the world's lamentable day,\n",
    "To watch the next way with his father with his face?\n",
    "\n",
    "ESCALUS:\n",
    "The cause why then we are all resolved more sons.\n",
    "\n",
    "VOLUMNIA:\n",
    "O, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, it is no sin it should be dead,\n",
    "And love and pale as any will to that word.\n",
    "\n",
    "QUEEN ELIZABETH:\n",
    "But how long have I heard the soul for this world,\n",
    "And show his hands of life be proved to stand.\n",
    "\n",
    "PETRUCHIO:\n",
    "I say he look'd on, if I must be content\n",
    "To stay him from the fatal of our country's bliss.\n",
    "His lordship pluck'd from this sentence then for prey,\n",
    "And then let us twain, being the moon,\n",
    "were she such a case as fills m\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While most of the sentences are grammatically correct, they do not make sense. But the model seems to have learned some attributes of the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And how many unique characters are there? Let's check it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = sorted(set(text))\n",
    "print ('{} unique characters'.format(len(vocab)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are some special symbols and characters (including lowercase and uppercase letters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vectorize the text\n",
    "Before training, we need to **map all the characters in the dataset to a numerical representation**. \n",
    "\n",
    "We will create two lookup tables: \n",
    "- one for mapping **characters to numbers** (char2idx)\n",
    "- another for **numbers to characters** (idx2char)\n",
    "\n",
    "And we can vectorize the text data using char2idx."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a mapping from unique characters to indices\n",
    "char2idx = \n",
    "\n",
    "idx2char = \n",
    "\n",
    "# 1D integer vector for all characters in the text data\n",
    "text_as_int = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(char2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(idx2char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(text_as_int)\n",
    "print(text_as_int.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/><br/>\n",
    "Also let's check how the first 13 characters from the dataset text are mapped to integers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_text_as_int = text_as_int[:13]\n",
    "\n",
    "print ('{} ---- characters mapped to int ---- > {}'.format(text[:13], first_text_as_int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print ('{} ---- ints mapped to characters ---- > {}'.format(first_text_as_int, idx2char[first_text_as_int]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The prediction task\n",
    "Our goal is to predict **the most probable following character** when given a character or a sequence of characters. Therefore, the **input to the model will be a sequence of characters** and the model will learn to **predict the output : the following character at each time step**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create training examples and targets\n",
    "For now, we will divide the text data into training sequences. Each input sequence will contain `seq_length` characters from the text. For each input sequence, the corresponding targets contain the same length of text, but shifted one character to the right.\n",
    "\n",
    "Therefore, the steps for making training input/target are as follows :\n",
    "1. Break all the text into chunks of `seq_length+1`.\n",
    "2. Input data is whole characters except the last character.\n",
    "3. Target data is whole characters except the first character.\n",
    "\n",
    "For example, let's say that `seq_length` is 4 and our training text is \"HELLO\".\n",
    "In this example, **the input sequence would be \"HELL\", and the target sequence \"ELLO\"**.\n",
    "\n",
    "<img src=\"images/teacher_forcing.png\" alt=\"Drawing\" style=\"width: 700px;\"/>\n",
    "\n",
    "To do this, first use the [`tf.data.Dataset.from_tensor_slices`](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/data/Dataset#from_tensor_slices) function to convert the text vector into a stream of character indices.\n",
    "- `tf.data.Dataset.from_tensor_slices`: Creates a Dataset whose elements are slices of the given tensors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make char dataset (in the form of integer)\n",
    "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
    "\n",
    "for i in char_dataset.take(20):\n",
    "    print(str(int(i))+\" : \"+str(idx2char[int(i)]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sequences with sequence length +1\n",
    "seq_length = 100\n",
    "sequences = char_dataset.batch(seq_length+1, drop_remainder=True)\n",
    "\n",
    "print(\"There are \"+str(len(sequences))+\" sequences of length \"+str(seq_length+1))\n",
    "print()\n",
    "\n",
    "for item in sequences.take(5):\n",
    "    print(len(idx2char[item.numpy()]))\n",
    "    print(item.numpy())\n",
    "    print(repr(''.join(idx2char[item.numpy()])))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/><br/>\n",
    "\n",
    "Now we have to convert above text into input data and target data. Note that target data must be shifted one character to the right.\n",
    "\n",
    "(Input data is whole characters except the last character, Target data is whole characters except the first character.)\n",
    "\n",
    "To do this, we will use `tf.data.Dataset.map`. When we give some function to `tf.data.Dataset.map` as a parameter, it will apply the function to all elements and then return them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plus_1(x):\n",
    "    return x+1\n",
    "    \n",
    "temp_dataset = tf.data.Dataset.range(1, 6)  # ==> [ 1, 2, 3, 4, 5 ]\n",
    "print(list(temp_dataset.as_numpy_iterator()))\n",
    "temp_dataset = temp_dataset.map(plus_1)\n",
    "print(list(temp_dataset.as_numpy_iterator()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/><br/>\n",
    "Here, let's define a function 'construct_input_target' which returns input data and target data as explained above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_input_target(chunk):\n",
    "\n",
    "    \n",
    "    return \n",
    "\n",
    "dataset = sequences.map(construct_input_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for input_example, target_example in dataset.take(5):\n",
    "    print ('Input data: ', repr(''.join(idx2char[input_example.numpy()])))\n",
    "    print ('Target data:', repr(''.join(idx2char[target_example.numpy()])))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>\n",
    "\n",
    "During the training, **each index of these vectors are processed as one time step**. For the input at time step 0, the model receives the index for \"F\" and tries to predict the index for \"i\" as the next character. At the next timestep, it does the same thing but the **RNN considers the previous step context in addition to the current input character**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (input_idx, target_idx) in enumerate(zip(input_example[:15], target_example[:15])):\n",
    "    print(\"Step {:4d}\".format(i))\n",
    "    print(\"  input: {} ({:s})\".format(input_idx, repr(idx2char[input_idx])))\n",
    "    print(\"  expected output: {} ({:s})\".format(target_idx, repr(idx2char[target_idx])))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create training batches\n",
    "We used `tf.data` to split the text into manageable sequences. But before feeding this data into the model, we need to **shuffle the data and pack it into batches**.\n",
    "\n",
    "[`tf.data.Dataset.shuffle`](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/data/Dataset#shuffle)(buffer_size, seed=None, reshuffle_each_iteration=None) : Randomly shuffles the elements of this dataset.\n",
    "\n",
    "[`tf.data.Dataset.batch`](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/data/Dataset#batch)(batch_size, drop_remainder=False) : Combines consecutive elements of this dataset into batches.\n",
    "\n",
    "Note that `tf.data.Dataset.shuffle` **doesn't shuffle characters within each sentence**, but the sentences in dataset will be shuffled by sentences.\n",
    "\n",
    "<img src=\"images/shuffle1.png\" alt=\"Drawing\" style=\"width: 600px;\"/>\n",
    "\n",
    "<br/>\n",
    "<br/>\n",
    "\n",
    "<img src=\"images/shuffle2.png\" alt=\"Drawing\" style=\"width: 600px;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch size\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# Buffer size to shuffle the dataset\n",
    "# (TF data is designed to work with possibly infinite sequences,\n",
    "# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
    "# it maintains a buffer in which it shuffles elements).\n",
    "BUFFER_SIZE = 10000\n",
    "\n",
    "# Shuffle the data and create batches (1 data = (100, 100) ==> 0:99, 1:100)\n",
    "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/><br/>\n",
    "\n",
    "We can see that each batch has 64 input sentences (each has 100 characters) and 64 target sentences (each has 100 characters). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for input_example_batch, target_example_batch in dataset.take(1):\n",
    "    print(input_example_batch, target_example_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/><br/>\n",
    "\n",
    "Also we can see that all the target sentences are shifted one character to the right ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for input_example_batch, target_example_batch in dataset.take(1):\n",
    "    print(input_example_batch[0])\n",
    "    print(target_example_batch[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build The GRU Model\n",
    "We will use `tf.keras.Sequential` to define the model. And for the model in this practice, three layers will be used:\n",
    "\n",
    "- `tf.keras.layers.Embedding`: The input layer. A trainable lookup table that will map the numbers of each character to a vector with embedding_dim dimensions;\n",
    "- [`tf.keras.layers.GRU`](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/layers/GRU): A type of RNN with size units=rnn_units (You can also use a LSTM layer here.)\n",
    "- `tf.keras.layers.Dense`: The output layer, with vocab_size outputs.\n",
    "\n",
    "<img src=https://miro.medium.com/max/2400/1*dhq14CzJijlqjf7IlDB0uw.png>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### About the GRU\n",
    "\n",
    "GRU is a variation of LSTM. GRU has some different attributes compared to vanilla LSTM.\n",
    "\n",
    "- The two state vectors $c_t$ and $h_t$ in the LSTM Cell are merged into one vector $h_t$.\n",
    "- There is only one gate controller $z_t$ that controls all input gates.\n",
    "- There is no output gate and the state vector $h_t$ is the output of GRU.\n",
    "\n",
    "You can see details about the GRU at the this [link](https://arxiv.org/abs/1406.1078).\n",
    "In this practice, we will use GRU since its operation is faster than LSTM and it has fewer parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Length of the vocabulary in chars\n",
    "vocab_size = 65 # len(vocab)\n",
    "\n",
    "# The embedding dimension\n",
    "embedding_dim = 256\n",
    "\n",
    "# Number of RNN units\n",
    "rnn_units = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import Sequential \n",
    "from tensorflow.keras.layers import Embedding, GRU, Dense \n",
    "\n",
    "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
    "    model = Sequential([\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    ])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(\n",
    "    vocab_size = len(vocab),\n",
    "    embedding_dim=embedding_dim,\n",
    "    rnn_units=rnn_units,\n",
    "    batch_size=BATCH_SIZE)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "<img src=https://www.tensorflow.org/text/tutorials/images/text_generation_training.png>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Try the untrained model\n",
    "Now we will run the untrained model to see how it behaves.\n",
    "\n",
    "First let's check the shape of the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for input_example_batch, target_example_batch in dataset.take(1):\n",
    "    example_batch_predictions = model(input_example_batch)\n",
    "    print(input_example_batch.shape, \"# (batch_size, sequence_length)\") \n",
    "    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/><br/>\n",
    "\n",
    "Also we can check the model's prediction by probability distribution. For example, we can see the model's prediction for the first sentence's fifth character.\n",
    "\n",
    "(Note that since the current model is not trained yet.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(example_batch_predictions[0][5])\n",
    "print(tf.nn.softmax(example_batch_predictions[0][5]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Free input length\n",
    "\n",
    "In our practice, the sequence length of the input in oyr dataset is 100 but the model can be run on inputs of any length, which is an advantage of the recurrent neural network which can handle inputs of variable length."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get actual predictions from the model, we need to sample from the output distribution to get actual character indices. This distribution is defined by the logits over the character vocabulary.\n",
    "\n",
    "Try it for the first example in the batch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_samples : determines how many characters to sample at each iteration\n",
    "\n",
    "print(input_example_batch[0])\n",
    "print()\n",
    "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
    "print(sampled_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This gives us a prediction for the next character index at each timestep.\n",
    "\n",
    "Now in order to check the predicted sentence of our untrained model, we will squeeze the `sampled_indices` and convert them into characters.\n",
    "- [`tf.squeeze`](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/squeeze): Removes dimensions of size 1 from the shape of a tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_indices = tf.squeeze(sampled_indices,axis=-1).numpy()\n",
    "\n",
    "print(sampled_indices.shape)\n",
    "print(sampled_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After sqeezing the `sampled_indices`, we got 1D vector that contains indicies of predicted characters.\n",
    "\n",
    "Let's decode this vector to see the text predicted by this untrained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Input: \\n\", repr(\"\".join(idx2char[input_example_batch[0]])))\n",
    "print()\n",
    "print(\"Next Char Predictions: \\n\", repr(\"\".join(idx2char[sampled_indices ])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>\n",
    "\n",
    "Since our model is not trained yet, it seems to just predict next character randomly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train the model\n",
    "At this point the problem can be treated as a standard classification problem. **Given the previous RNN state and the input character at each time step, our model must predict the next character.**\n",
    "\n",
    "#### Compile the model\n",
    "We will use `tf.keras.losses.sparse_categorical_crossentropy` loss function since it works well for classification problem.\n",
    "\n",
    "Since our model returns logits, we need to set the `from_logits` flag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(labels, logits):\n",
    "    return \n",
    "\n",
    "model.compile(optimizer='adam', \n",
    "              loss=loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Configure checkpoints\n",
    "Use a [`tf.keras.callbacks.ModelCheckpoint`](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/callbacks/ModelCheckpoint) to ensure that checkpoints are saved during training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory where the checkpoints will be saved\n",
    "checkpoint_dir = './training_checkpoints'\n",
    "# Name of the checkpoint files\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
    "\n",
    "checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_prefix,\n",
    "    save_weights_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(dataset, \n",
    "                    epochs=15, \n",
    "                    callbacks=[checkpoint_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/training_result.png\" alt=\"Drawing\" style=\"width: 600px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate text\n",
    "We will restore the latest checkpoint. Then, to keep this prediction step simple, we will use 1 for batch size.\n",
    "\n",
    "(Note that in order to run the model with a different `batch_size`, we need to rebuild the model with different batch size and restore the weights from the checkpoint.)\n",
    "\n",
    "By the codes below, we can check the path that contains weights for the lastest model and load it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rebuild the model by changing batch size (=1) to predict new text\n",
    "model = build_model(vocab_size, embedding_dim, rnn_units, batch_size=1)\n",
    "\n",
    "# Load the weight of the model we trained \n",
    "# model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
    "model.load_weights('./saved_ckpt/ckpt_15')\n",
    "\n",
    "# Change the batch size from 64 to 1\n",
    "model.build(tf.TensorShape([1, None]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The prediction loop\n",
    "\n",
    "1. Start by choosing a **start string** and initialize the RNN hidden state for the first iteration.\n",
    "2. Set the number of characters to generate.\n",
    "3. Get the **prediction distribution of the next character using the start string and hidden state**.\n",
    "4. Sample an index of the predicted character using a multinomial distribution of the first iteration. \n",
    "5. Use this predicted character as our next input to the model.\n",
    "6. Repeat step 3-5 until we get the number of characters we set.\n",
    "\n",
    "**Note that the RNN hidden state returned by the model is fed back into the model and hidden state will become more complex as the prediction loop repeats.**\n",
    "In other words, after predicting the a word, the modified RNN states are again fed back into the model, which is how the model learns as it gets more context from the previously predicted words.\n",
    "\n",
    "\n",
    "![To generate text the model's output is fed back to the input](https://www.tensorflow.org/text/tutorials/images/text_generation_sampling.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, start_string, num_generate,temperature):\n",
    "    # Evaluation step (generating text using the learned model)\n",
    "\n",
    "    # Converting our start string to numbers (vectorizing)\n",
    "    input_eval = [char2idx[s] for s in start_string]\n",
    "    input_eval = tf.expand_dims(input_eval, 0)\n",
    "\n",
    "    # Empty string to store our results\n",
    "    text_generated = []\n",
    "\n",
    "    # Here batch size == 1\n",
    "    model.reset_states()\n",
    "    for i in range(num_generate):\n",
    "        predictions = model(input_eval)\n",
    "        # remove the batch dimension\n",
    "        predictions = tf.squeeze(predictions, 0)\n",
    "\n",
    "        # using a categorical distribution to predict the word returned by the model\n",
    "        predictions = predictions / temperature\n",
    "        predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
    "\n",
    "        # We pass the predicted word as the next input to the model\n",
    "        # along with the previous hidden state\n",
    "        input_eval = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "        text_generated.append(idx2char[predicted_id])\n",
    "\n",
    "    return (start_string + ''.join(text_generated))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Low temperatures results in more predictable text.\n",
    "# Higher temperatures results in more surprising text.\n",
    "# Experiment to find the best setting.\n",
    "\n",
    "print(generate_text(model, start_string=\"I will go home and sleep\", num_generate = 500, temperature=1.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/><br/>\n",
    "\n",
    "Looking at the generated text, you'll see the model knows when to capitalize and make paragraphs, and it imitates a Shakespeare-like writing vocabulary.\n",
    "\n",
    "<br/>\n",
    "The easiest thing you can do to improve the results is to train it for longer (e.g. try EPOCHS=30).\n",
    "\n",
    "You can also experiment with a different start string, or try adding another RNN layer to improve the model's accuracy, or adjusting the temperature parameter to generate more or less random predictions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
