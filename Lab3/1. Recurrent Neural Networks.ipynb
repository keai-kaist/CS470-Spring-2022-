{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS470 Introduction to Artificial Intelligence\n",
    "## Deep Learning Practice \n",
    "#### Prof. Ho-Jin Choi\n",
    "#### School of Computing, KAIST\n",
    "\n",
    "---\n",
    "#### Contents\n",
    "- Introduction to RNNs\n",
    "- Word Embeddings\n",
    "- Text classification with an RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Recurrent Neural Networks\n",
    "### 4-1. Introduction to RNNs\n",
    "#### Why RNN?\n",
    "\n",
    "![Sequential data](images/sequential-data.PNG)\n",
    "\n",
    "Traditional neural networks such as FFNN, CNN have no **persistence properties** when predicting some results (e.g., as you read this sentence, you understand each word based on your understanding of previous words). This is a major shortcoming of the neural networks. \n",
    "\n",
    "![Recurrent Neural Network](images/rnn.png)\n",
    "\n",
    "Recurrent neural networks address this issue. They are networks with loops in them, allowing information to persist. RNN is a class of artificial neural network where connections between units form a directed graph along a temporal sequence. This allows it to exhibit temporal dynamic behavior. Unlike feedforward neural networks, RNNs can use their internal state (memory) to process sequences of inputs. This makes them applicable to tasks such as sentence generation, machine translation, image captioning or speech recognition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vanishing Gradient Problem\n",
    "![RNN vanishing gradients](images/rnn-vanishing-gradient.png)\n",
    "Vanishing gradient problem is a difficulty found in training artificial neural networks with gradient-based learning methods and backpropagation. In such methods, each of the neural network's weights receives an update proportional to the partial derivative of the error function with respect to the current weight in each iteration of training. \n",
    "\n",
    "\n",
    "The problem is that in some cases, **the gradient will be vanishingly small**, effectively preventing the weight from changing its value. **In the worst case**, this **may completely stop the neural network from further training**. \n",
    "\n",
    "![vanilla RNN](images/vanilla-rnn.PNG)\n",
    "![sigmoid and its derivative](images/sigmoid-and-sigmoid-derivative.png)\n",
    "In the case of RNN, the above is the basic equation of the RNN, and it uses sigmoid funciton as activation. Traditional activation functions such as the sigmoid function have gradients in the range (0, 0.25), and backpropagation computes gradients by the chain rule. This has the effect of **multiplying $n$ of these small numbers throughout the time steps**. Therefore, the gradient (error signal) decreases exponentially with $n$ and model cannot learn long-term dependencies between distant words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Example of vanishing gradient\n",
    "- Sentence 1: That **_cat_**, which already ate ..., **_was_** full.\n",
    "- Sentence 2: That **_cats_**, which already ate ..., **_were_** full.\n",
    "\n",
    "This is one example when language can have long term dependencies. Like in above example **_cat_** which came early in the sentence can affect what came later.\n",
    "\n",
    "![Example of vanishing gradient](images/example-of-vanishing-gradient.png)\n",
    "\n",
    "However, the basic recurrent neural network is not very good in remembering long term dependencies. The output in this network can mostly be affected by the value close to it. Thus, $\\hat{y}^{<3>}$ is mainly influenced by values close to $\\hat{y}^{<3>}$ ($x^{<1>}$, $x^{<2>}$, $x^{<3>}$).\n",
    "\n",
    "In other words, in the case of $\\hat{y}^{<T_y>}$, this cannot be influenced by the early inputs in the sequences ($x^{<1>}$, $x^{<2>}$, $x^{<3>}$) because it is hard for the error to be backpropagated to the beginning of the sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Long short-term memory (LSTM)\n",
    "\n",
    "![Long short-term memory](images/lstm.png)\n",
    "\n",
    "Long Short-Term Memory networks – usually just called \"LSTMs\" – are a special kind of RNN, capable of learning long-term dependencies. The key idea of LSTM is an additive connection of previous memories passed through time. They work tremendously well on a large variety of problems, and are now widely used. LSTMs are explicitly designed to avoid the long-term dependency problem. Remembering information for long periods of time is practically their default behavior.\n",
    "\n",
    "![Long short-term memory](images/lstm-vanishing-gradient.jpg)\n",
    "\n",
    "A common LSTM unit is composed of a **cell**, an **input gate**, an **output gate** and a **forget gate**. The cell is responsible for _\"remembering\" values over arbitrary time intervals_; hence the word \"memory\" in LSTM. Each of the three gates can be thought intuitively as regulators of the flow of values that goes through the connections of the LSTM.\n",
    "\n",
    "\n",
    "For more detailed LSTM:\n",
    "https://colah.github.io/posts/2015-08-Understanding-LSTMs/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
