{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS470 Introduction to Artificial Intelligence\n",
    "## Deep Learning Practice\n",
    "#### TA. Jonghwa Lee / jongwhoa.lee@kaist.ac.kr\n",
    "#### Prof. Ho-Jin Choi\n",
    "#### School of Computing, KAIST\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 2 - Convolutional Neural Networks (CNNs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. **Basic of CNN** <br>\n",
    "    3-1. Convolutional neural network <br>\n",
    "    3-2. Layers composing CNN <br>\n",
    "    3-3. Data Augmentation <br>\n",
    "    3-4. Image classification using CNN\n",
    "    \n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Convolutional Neural Network\n",
    "### 3-1. Convolutional neural network\n",
    "- A convolutional neural networks (CNN) is a class of deep neural networks, most commonly applied to analyzing visual tasks.\n",
    "- The networks consist of multiple layers of small neuron collections which process portions of the input image, called receptive fields.\n",
    "    - The connectivity pattern between these neurons is inspired by the organization of the animal visual cortex. <br>\n",
    "![fnn_vs_cnn](images/fnn_vs_cnn.png)\n",
    "- The outputs of these collections are then tiled so that their input regions overlap, to obtain a better representation of the original image; this is repeated for every such layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example of CNN architecture\n",
    "![Convolutional neural network](images/cnn-architectures.png)\n",
    "\n",
    "- CNN consists of a series of convolutional, non-linear, pooling (downsampling), and fully connected layers.\n",
    "- The convolutional, non-linear and pooling layers are to extract a feature map (or activation map).\n",
    "- The fully connected layer is to classify a target using the extracted feature map (e.g. classification a single class of input image or a probability of classes that best describes the image in image classification)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-2. Layers composing CNN\n",
    "#### Convolutional layer\n",
    "The first layer in a CNN is usually a **convolutional layer**.\n",
    "\n",
    "![Convolution layer](images/convolution-layer.gif)\n",
    "\n",
    "<img src=https://taewanmerepo.github.io/2018/01/cnn/conv2.jpg width=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Convolutional filters\n",
    "\n",
    "![Convolution filter](images/convolution-filter.png)\n",
    "\n",
    "- A convolutional filter much like a **kernel** in image recognition is a small matrix useful for blurring, sharpening, embossing, edge detection, and more.\n",
    "- This is accomplished by computing a convolution between an filter and an image.\n",
    "- The main difference **_here_** is that the **convolution matrices are learned**.\n",
    "    - This means that the convolutional filters are learned as a weights during training the CNN models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Convolution\n",
    "As the filter is sliding, or convolving, around the input image, it multiplies the values in the filter with the original pixel values of the image (a.k.a. computing element-wise multiplication) and summed up all these multiplications.\n",
    "\n",
    "![Activation map](images/activation-map.gif)\n",
    "\n",
    "Now, we repeat this process for every location on the input volume. (Next step would be moving the filter to the right by 1 unit, then right again by 1, and so on.) After sliding the filter over all the locations, we are left with an array of numbers usually called an activation map or feature map."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Stride\n",
    "- It controls how the filter convolves around the input volume.\n",
    "- In the above example, the filter convolves around the input volume by shifting one unit at a time.\n",
    "- In that case, the stride was implicitly set at 1.\n",
    "- Stride is normally set in a way so that the output volume is an integer and not a fraction.\n",
    " \n",
    "![Stride - 1](images/stride-1.gif)\n",
    "![Stride - 2](images/stride-2.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Padding\n",
    "- The size of the feature map is smaller than the input, because the convolution filter needs to be contained in the input. \n",
    "- If we want to maintain the same dimensionality, we can use padding to surround the input with zeros.\n",
    " \n",
    "![Activation map](images/padding.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**More details for computation of convolutional filter (reference material)**\n",
    "\n",
    "##### High level perspective\n",
    "Let's talk about briefly what this convolution is actually doing from a high level. Each of these filters can be thought of as **feature identifiers** (e.g. *straight edges, simple colors, curves*)\n",
    "\n",
    "![Filter is feature identifier](images/filter-is-feature-identifier.png)\n",
    "\n",
    "###### Visualisation of the Receptive Field\n",
    "![Example image and filter](images/example-image-and-filter.png)\n",
    "![Good filter](images/good-filter.png)\n",
    "![Bad filter](images/bad-filter.png)\n",
    "\n",
    "Compared to the first value, the second value is much lower! This is because there wasn’t anything in the image section that responded to the curve detector filter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ReLU (Rectified Linear Units) layer\n",
    "Conventionally, there are other layers that are interspersed between these convolutional layers.  \n",
    "(e.g. Input → Conv → ReLU → Conv → ReLU → Pool → Conv → ReLU → Conv → ReLU → Pool → Fully connected)\n",
    "\n",
    "After each convolutional layer, the **ReLU layer** is immediately added to apply a nonlinear layer. The purpose of this layer is to introduce nonlinearity to a system that basically has just been computing linear operations during the convolutional layers (just element-wise multiplications and summations)\n",
    "\n",
    "#### ReLU function (Recap)\n",
    "- The **ReLu** function is defined as $f(x) = \\max(0, x)$.\n",
    "![Relu](images/relu50.png)\n",
    "\n",
    "#### The advantages of ReLU layer\n",
    "- It works far better than other nonlinear functions (e.g. tanh and sigmoid), because the network is able to train a lot faster (due to computational efficiency) without making a significant difference to the accuracy.\n",
    "- It also helps to alleviate the vanishing gradient problem, which is the issue where the lower layers of the network train very slowly because the gradient decreases exponentially through the layers.\n",
    "    - Sigmoid function has a maximum slope of 0.25. During backward propagation, The gradient will be multiplied with values less than 1. And if layer is deeper, the gradient will exponentially decrease by multipling value with less than 1. RELU activation solves this by having a gradient slope of 1, so during backpropagation.  \n",
    "![sigmoid_tanh_with_derivate](images/sigmoid_tanh_with_derivate.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pooling layer\n",
    "\n",
    "After some ReLU layers, it is customary to apply a pooling layer (a.k.a. *downsampling layer*). In the pooling layers, there are also several options, with **maxpooling** being the most popular. (There are other options such average pooling and L2-norm pooling.)\n",
    "\n",
    "![Max pool](images/max-pool.png)\n",
    "\n",
    "##### The purpose of pooling layer\n",
    "- To reduce the amount of parameters\n",
    "    - The pooling layer drastically reduces the spatial dimension (the length and the width but not the depth) of the input volume.\n",
    "    - By reducing the diemnsion, there remain more important features of an input image.\n",
    "- To control an overfitting\n",
    "    - Because reduce the amount of paraeters and complexity of a model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fully-connected layer\n",
    "The last layer is an important one, namely the fully-connected layer. Basically, a fully-connected layer looks at what high level features most strongly correlate to a particular class. It has particular weights so that when you compute the products between the weights and the previous layer, we can get the correct probabilities for the different classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-3. Data Augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Augmentation  \n",
    "- In real world, the data is limited.\n",
    "- Due to insufficient data, training model may suffer from overfitting.\n",
    "- Increases the training data to prevent overfitting  \n",
    "\n",
    "#### Horizontal flip\n",
    "\n",
    "![Horizontal Flip](images/horizontal_flip.png)\n",
    "\n",
    "#### Random crop  \n",
    "- Increase the image size slightly more than input size,  and crop the images at random locations\n",
    "![Random Crop](images/random-crop.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SimCLR\n",
    "- Powerful and data effectiveframework using data augmentation.\n",
    "- When SimCLR trained on 1% of labels of ImageNet dataset, it achieves 85.8% top-5 accuracy\n",
    "![simclr actitecture](images/simclr-architecture.png)\n",
    "Chen at al, A simple framework for contrastive learning of visual representations, PMLR, 2020\n",
    "\n",
    "#### Illustration of SimCLR\n",
    "<img src = \"images/simCLR-gif.gif\" align=\"center\" width=\"60%\">\n",
    "\n",
    "\n",
    "#### ImageNet top-1 accuracy\n",
    "![SimCLR-Acc](images/SimCLR-perfomance.png)\n",
    "\n",
    "[Reference for SimCLR](https://amitness.com/2020/03/illustrated-simclr/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Going further: Convolution Arithmetic\n",
    "\n",
    "If you want to go further with Convolution and you want to fully understand how convolution works with all the details we omitted in this notebook, I strongly suggest to read this terrific paper: \n",
    "- [A guide to convolution arithmetic for deep learning](https://arxiv.org/abs/1603.07285).\n",
    "- [A simple framework for contrastive learning of visual representations](http://proceedings.mlr.press/v119/chen20j/chen20j.pdf)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
