{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS470 Introduction to Artificial Intelligence\n",
    "## Deep Learning Practice \n",
    "#### TA. Jinsu Lim\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2-2. Text classification using IMDB dataset\n",
    "\n",
    "#### Topics for this chapter\n",
    " * IMDB datset\n",
    " * One-hot Encoding\n",
    " * Overfitting\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### IMDB dataset\n",
    "\n",
    "The IMDB dataset is data that contains movie review comments and positive or negative label representing the evaluation of the audience. This contains the text comments for 50,000 movie reviews from the [Internet Movie Database](https://www.imdb.com/). These data are splited in half so that we can get 25,000 reviews for training and 25,000 reviews for testing. The training and testing sets are balanced and unbiased, which means they contain an equal number of positive and negative reviews.\n",
    "\n",
    "#### Importance of balanced dataset\n",
    "Let's assume that we have 9,999 positive reviews and 1 negative review. Without any training, we can get 99.99% accuracy by just predicting all reviews as positive. This situation is called the _Accuracy Paradox_. Since we're using accuracy as a model metric, it's important to **balance the dataset well**. In the case of MNIST dataset, we can say that the dataset is balanced when we have about 60,000/10=6,000 images per digit class. (60,000 is the total number of images and 10 is the number of class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "try:\n",
    "  # %tensorflow_version only exists in Colab.\n",
    "  %tensorflow_version 2.x\n",
    "except Exception:\n",
    "    pass\n",
    "import tensorflow as tf\n",
    "print('Tensorflow: ', tf.__version__)\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To handle the text dataset, we first need to transform the data into a form that can be used for training. There are two common methods we can use when dealing with text data, **one-hot encoding** and **word embedding**. In this practice, we'll use one-hot encoding method (Word embedding will be covered later in the RNN part). Before using one-hot encoding, let's take a quick look at what one-hot encoding is.\n",
    "\n",
    "<img src=\"images/one_vs_we.png?raw=true\" width=1000>\n",
    "<br/><br/>\n",
    "\n",
    "#### One-hot encoding\n",
    "One-hot encoding is a vector representation of sentences. It has the **vector dimension as the size of the word set** and **1 is assigned to the index of the word to be expressed / 0 is assigned to the other index**. The detailed procedure of one-hot encoding is as follows.\n",
    "\n",
    "(1) Give each word an unique index.\n",
    "\n",
    "(2) 1 is assigned to the index of the word to be expressed, and 0 is assigned otherwise.\n",
    "\n",
    "<br/>\n",
    "I will explain this method with simple example.\n",
    "<br/>\n",
    "Let's assume that we have a word dictionary that contains 7 words, 'today' ,'I' ,'was' ,'studied' ,'happy' ,'math' ,'not'. We can map these words to integers 0-6. \n",
    "\n",
    "<br/>\n",
    "<img src=\"images/onehot.png?raw=true\" width=300>\n",
    "<br/>\n",
    "\n",
    "Now, we can represent sentences consisting of words from the dictionary. \n",
    "\n",
    "For example, a sentence 'I studied math today' can be represented as [0, 1, 3, 5] (not considering the order).\n",
    "If we make a vector that has the vector dimension as the size of the dictionary, we get a vector as below.\n",
    "\n",
    "<img src=\"images/empty.png?raw=true\" width=300>\n",
    "\n",
    "After assining 1 to the index of the words in the original list ([0, 1, 3, 5]), we get one-hot encoded vector as below.\n",
    "\n",
    "<img src=\"images/convert.png?raw=true\" width=300>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Limitation of One-hot encoding\n",
    "One-hot encoding has the disadvantage that if the number of words increases, the space required to store the vector increases too. If we have 10,000 words that we want to express, regardless of the length of the sentence to be expressed, a vector of length 10,000 is required. (even for a sentence that contains only one word.)\n",
    "\n",
    "Also, the one-hot vector encoding cannot express the similarity of words. Assume that we have 4 words 'wolf', 'dog', 'tiger', 'chair'. We can consider 'wolf', 'dog', 'tiger' have semantically similar context since they are all animals and 'chair' is less similar to the previous three words. In one-hot encoding, since inner-product similarity between two different words is always 0 and Euclidean distance between them is always $\\sqrt{2}$, there is no way to express the degree of similarity between words.\n",
    "\n",
    "Despite these shortcomings, one-hot encoding is easy to understand and use. Another reason for using the one-hot encding method is to check the overfitting problem in next practice. Therefore in this practice, we will use one-hot encoding.\n",
    "Let's first download the IMDB dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_WORDS = 10000\n",
    "(train_data, train_labels), (test_data, test_labels) = tf.keras.datasets.imdb.load_data(num_words=NUM_WORDS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The argument num_words=10000 keeps the top 10,000 most frequently occurring words in the training data. The rare words are discarded to keep the size of the data manageable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a moment to understand the format of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training entries: {}, labels: {}\".format(len(train_data), len(train_labels)))\n",
    "print(\"Train data[0]: {}. \\nlabel[0]: {}\".format(train_data[0], train_labels[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset comes preprocessed: each example is an array of integers representing the words of the movie review. Each label is an integer value of either 0 or 1, where 0 is a negative review, and 1 is a positive review.\n",
    "\n",
    "From each data, we'll create an ont-hot encodering vector with a 10,000-dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_hot_sequences(sequences, dimension):\n",
    "    # Create an all-zero matrix of shape (len(sequences), dimension)\n",
    "    results = np.zeros((len(sequences), dimension))\n",
    "    for i, word_indices in enumerate(sequences):\n",
    "        results[i, word_indices] = 1.0  # set specific indices of results[i] to 1s\n",
    "    return results\n",
    "\n",
    "NUM_WORDS = 10000\n",
    "\n",
    "train_data = multi_hot_sequences(train_data, dimension=NUM_WORDS)\n",
    "test_data = multi_hot_sequences(test_data, dimension=NUM_WORDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Train data[0]: {}. \\nlabel[0]: {}\".format(train_data[0], train_labels[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build model\n",
    "\n",
    "Similar to previous model, we'll define a simple model using the [`tf.keras.layers.Dense`](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/layers/Dense) layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: define a sequential model\n",
    "model = tf.keras.Sequential([\n",
    "    # TODO: add a first dense layer with 16 nodes applying ReLU activation function\n",
    "    \n",
    "    # TODO: add a second dense layer with 16 nodes applying ReLU activation function\n",
    "    \n",
    "    # TODO: add a last dense layer with only 1 node applying Sigmoid activation function\n",
    "    \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compile the model\n",
    "\n",
    "After defining the model structure, we configure the model such as optimizer, loss and metrics for training. We will use loss function as `binary_crossentropy` which is popular for binary classification problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', \n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a validation set\n",
    "\n",
    "When training, we want to check the accuracy of the model on data it hasn't seen before. So we will create a **validation set** by setting apart 10,000 examples from the original training data. The reason that we do not use the testing set now is our goal is to develop and tune our model using only the training data and then use the test data just once to evaluate our accuracy. Splitting whole data into three sub-data (training/validation/test) is very common."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First 10,000 elements of train_data\n",
    "x_val =\n",
    "\n",
    "# Rest elements of train_data\n",
    "partial_x_train =\n",
    "\n",
    "\n",
    "# First 10,000 elements of train_labels\n",
    "y_val = \n",
    "\n",
    "# Rest elements of train_labels\n",
    "partial_y_train ="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train the model\n",
    "\n",
    "Then, we'll start to train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the batch size and the number of epochs to use during training\n",
    "BATCH_SIZE = 512\n",
    "EPOCHS = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: train the model using (train_images, train_labels). \n",
    "#       do not forget to set epochs and batch size as the parameters!\n",
    "model_history = model.fit(_________,\n",
    "                          _________,\n",
    "                          epochs=_________,\n",
    "                          batch_size=_________,\n",
    "                          validation_data=(____, ____),\n",
    "                          verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this approach our model reaches a validation accuracy of around 88% (note the model is overfitting, training accuracy is significantly higher)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate the model\n",
    "And let's see how the model performs. Two values will be returned, loss (a number that represents our error, lower values are better) and accuracy (higher values are better)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = model.evaluate(test_data, test_labels, verbose=0)\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This fairly naive approach achieves an accuracy of about 85%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plotting network performance trend\n",
    "Again let's plot the trend of loss and accuracy of our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_dict = model_history.history\n",
    "history_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_dict = model_history.history\n",
    "\n",
    "acc = history_dict['accuracy']\n",
    "val_acc = history_dict['val_accuracy']\n",
    "loss = history_dict['loss']\n",
    "val_loss = history_dict['val_loss']\n",
    "\n",
    "epochs = range(1, len(acc) + 1)\n",
    "\n",
    "plt.figure(figsize=(12,9))\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(12,9))\n",
    "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc',color='green')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.ylim((0.5,1))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this plot, the dots represent the training loss and accuracy, and the solid lines are the validation loss and accuracy.\n",
    "\n",
    "Notice that the training loss decreases with each epoch and the training accuracy increases with each epoch. This is expected result when using a gradient descent optimizationâ€”based method. It will minimize the loss function value on every iteration.\n",
    "\n",
    "However, this isn't the case for the validation loss and accuracy. Validation accuracy seems to peak after about very few epochs and validation loss goes up after that point. This is an example of **_overfitting_: the model performs better on the training data than it does on data it has never seen before.** After this point, the model over-optimized and learned representations specific to the training data that do not generalize to test data.\n",
    "\n",
    "To cope with this situation, we can prevent overfitting by simply stopping the training after few epochs. And in the next practice, we will learn some techniques to prevent overfitting and make model with generality."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
