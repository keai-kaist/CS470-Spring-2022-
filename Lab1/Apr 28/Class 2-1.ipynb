{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS470 Introduction to Artificial Intelligence\n",
    "## Deep Learning Practice \n",
    "#### TA. Jinsu Lim\n",
    "---\n",
    "\n",
    "### Agenda for today\n",
    "#### 2-1. MLP-based Classification\n",
    "#### 2-2. Text classification using IMDB dataset\n",
    "#### 2-3. Overfitting and how to fight it\n",
    "#### 2-4. Save and restore models\n",
    "---\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 2-1. MLP-based Classification\n",
    "\n",
    "#### Topics for this chapter\n",
    " * MNIST datset\n",
    " * Build/Compile/Train the model\n",
    " * Hyperparameter\n",
    " * Plot metrics\n",
    " * Evaluate/Test test data\n",
    " * Visualize the results\n",
    "---\n",
    "\n",
    "### Image classification using MNIST dataset\n",
    "\n",
    "In this practice, we will build our first fully connected feed-forward network. \n",
    "We will construct a neural network consisting of two fully connected layers and apply this model to the digit classification task. \n",
    "Our network will ultimately output **a probability distribution over the 10 digit classes (0-9)**. \n",
    "The architecture of the network we are going to build is depicted below:\n",
    "\n",
    "![Architecture - MNIST](images/architecture-mnist.png?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MNIST dataset\n",
    "\n",
    "The [MNIST dataset](http://yann.lecun.com/exdb/mnist) is a large database of handwritten digits that is commonly used for training various image processing systems. This dataset consists of 60,000 training images and 10,000 test images. The target classes for this dataset are the digits (0-9). \n",
    "\n",
    "Let's download and load the dataset and display a few random samples from it. We can load them by mnist module in keras.datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "try:\n",
    "  # %tensorflow_version only exists in Colab.\n",
    "  %tensorflow_version 2.x\n",
    "except Exception:\n",
    "    pass\n",
    "import tensorflow as tf\n",
    "print('Tensorflow: ', tf.__version__)\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = tf.keras.datasets.mnist\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
    "print(train_images.shape)\n",
    "\n",
    "train_images = np.expand_dims(train_images, axis=-1) / 255\n",
    "print(train_images.shape)\n",
    "\n",
    "train_labels = np.int64(train_labels)\n",
    "test_images = np.expand_dims(test_images, axis=-1) / 255\n",
    "test_labels = np.int64(test_labels)\n",
    "class_names = ['0','1','2','3','4','5','6','7','8','9']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our training set is made up of 28x28 grayscale images of handwritten digits. Let's visualize some of these images and check their corresponding training labels.\n",
    "\n",
    "By using the written codes below, we can check randomly sampled 36 images and their corresponding labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "random_indicies = np.random.choice(60000, 36)\n",
    "\n",
    "for index in range(36):\n",
    "    plt.subplot(6, 6, index + 1)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.grid(False)\n",
    "    image_index = random_indicies[index]\n",
    "    plt.imshow(np.squeeze(train_images[image_index]), cmap=plt.cm.binary)\n",
    "    plt.xlabel(train_labels[image_index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build model\n",
    "\n",
    "To construct the architecture of the fully connected neural network, we'll once again use the Keras API and define the model using the [`tf.keras.Sequential`](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/Sequential) class. Note that we first use a [`tf.keras.layers.Flattern`](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/layers/Flatten) layer, which flattens the input so that the flatten vector can be fed into the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: define a sequential model\n",
    "model = tf.keras.Sequential([\n",
    "    # TODO: add a flatten layer\n",
    "    \n",
    "    # TODO: add the first dense layer with 128 nodes applying ReLU activation function\n",
    "    \n",
    "    # TODO: add the last dense layer with 10 (the number of digits) nodes applying Softmax activation function\n",
    "    \n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Flatten](images/flatten.png?raw=true)\n",
    "\n",
    "Let's take a step back and think about the network we've just made. The first layer in this network, `tf.keras.layers.Flatten`, transforms the format of the images from a 2d-array (28 x 28 pixels), to a 1d-array of 28 * 28 = 784 pixels. You can think of this layer as unstacking rows of pixels in the image and lining them up. There are no trainable parameters in this layer and it only reshapes the data.\n",
    "\n",
    "After the pixels are flattened, the network consists of a sequence of two `tf.keras.layers.Dense` layers. These are fully connected neural network layers. \n",
    "Each dense layer has following activation function.\n",
    "\n",
    "- The second `Dense` layer has 128 nodes (or neurons)\n",
    "    - ReLU $ f(x) = \\max(0, x) $ activation function is applied to an output of this layer.\n",
    "        - This introduces **non-linearity** in the input space.\n",
    "        - By this non-linearity, the network can learn to capture complicated (linearly non-separable) relationships.\n",
    "- The last `Dense` layer has 10 nodes (or neurons)\n",
    "    - Softmax $ f_j(z) = \\frac{e^{z_j}}{\\sum_{K} e^{z_k}} $ activation function is applied to an output of this layer.\n",
    "        - This takes a vector of arbitrary real-valued scores and squashes it to **a vector of values between zero and one that sum to one**, which can be interpreted as probability.\n",
    "        - That vector can represent a probability distribution over predicted output classes given the input value.\n",
    "        \n",
    "\n",
    "<br/>\n",
    "<img src=\"images/softmax.jpeg?raw=true\" width=500>\n",
    "<br/>\n",
    "\n",
    "\n",
    "By calling `summary()`, we can check the models architecture briefly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Wait! Can you derive the number of parameters of each layer?_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compile the model\n",
    "\n",
    "Before training the model, we need to define a few more settings. These settings will be applied during the model's [`compile`](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/Model) step:\n",
    "\n",
    "- *Loss function*: This defines a measure of how accurate our model is. During training, we want to minimize this function, which will \"steer\" the model in the right direction.\n",
    "- *Optimizer*: This defines how the model is updated based on the data it sees and its loss function.\n",
    "- *Metrics*: Here we can define metrics used to monitor the training and testing steps.\n",
    "\n",
    "We'll start out by using _adam_ optimizer initialized with a learning rate of 0.1. Since we are performing a categorical classification task, we will use the [sparse_categorical_crossentropy](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/losses/sparse_categorical_crossentropy).\n",
    "\n",
    "You can experiment with other the choice of optimizer and learning rate and then evaluate how these affect the accuracy of the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: compile the model with the following parameters\n",
    "# - Optimizer: adam optimizer\n",
    "# - Loss: sparse_categorical_crossentropy \n",
    "# - Metrics: accuracy\n",
    "\n",
    "model.compile(\n",
    "    \n",
    "####\n",
    "\n",
    "    \n",
    "####\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train the model\n",
    "\n",
    "We're now ready to train our model. Now we will feed the training data `(train_images, train_labels)` into the model, and then ask it to learn the associations and relationships between images and labels. We'll also need to define the batch size, the number of epochs or iterations (some hyperparameters) over the MNIST dataset to specify the training. After specifying the model settings with the Keras API in the `compile` step, training can be started by calling the [`fit`](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/Model#fit) method on an instance of the `Model` class. \n",
    "\n",
    "#### Hyperparameter\n",
    "In machine learning, hyperparameters are parameters with values that are used to control the learning process. In contrast to the parameters within the model, the values of hyperparameters are not trainable. Number of layers and size of model can be also considered as hyperparameters. By changing hyperparameters and monitoring the metrics, we can choose proper hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the batch size and the number of epochs to use during training\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: train the model using (train_images, train_labels). \n",
    "#       do not forget to set epochs and batch size as the parameters!\n",
    "\n",
    "network_history = model.fit(\n",
    "    \n",
    "####\n",
    "    \n",
    "    \n",
    "####\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the model trains, the loss and accuracy metrics are displayed. With ten epochs and a learning rate of 0.01, this fully connected model achieved an accuracy of approximatley 0.98 (or 98%) on the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plotting network performance trend\n",
    "\n",
    "The return value of the `fit` function is a `tf.keras.callbacks.History` object which contains the entire history of training/validation loss and accuracy, for each epoch. We can therefore plot the trend of loss and accuracy of our model during the training phase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_dict = network_history.history\n",
    "history_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(network_history):\n",
    "    plt.figure()\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.plot(network_history.history['loss'])\n",
    "    plt.legend(['Training'])\n",
    "\n",
    "    plt.figure()\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.plot(network_history.history['accuracy'])\n",
    "    plt.legend(['Training', 'Validation'], loc='lower right')\n",
    "    plt.show()\n",
    "\n",
    "plot_history(network_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate accuracy on the test dataset\n",
    "\n",
    "Now we've trained the model and we can use the trained model to make predictions about test dataset which hasn't seen before to the model. In our practice, the `test_images` array comprises our test dataset. To evaluate accuracy, we can check whether the model's predictions match with the ground truth labels from the `test_labels` array. \n",
    "\n",
    "We will use the [`evaluate`](https://tensorflow.google.cn/api_docs/python/tf/keras/models/Sequential#evaluate) method to evaluate the model on the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_accuracy = model.evaluate(test_images, test_labels, verbose=2)\n",
    "print('Test accuracy:', test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may observe that the accuracy on the test dataset is a little lower than the accuracy on the training dataset. This gap between training accuracy and test accuracy is an example of *overfitting*, which occurs when a trained model performs worse on new (test) data than on its training data (we will talk about _overfitting_ in the next practice)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make predictions\n",
    "With the trained model, let's observe some predictions about in the test images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(test_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, the model has predicted the label for each image in the testing set. Let's take a look at the first prediction output of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(predictions[0])\n",
    "plt.imshow(np.squeeze(test_images[0]), cmap=plt.cm.binary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each decimal number in the array represents the confidence level of the model's prediction.\n",
    "\n",
    "This time, let's check which number the model has predicted by looking at the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Predicted digit for test image[0]: {}'.format(np.argmax(predictions[0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, the model is most confident that this image is a \"7\". We can check the test label (remember, this is the true identity of the digit) to see if this prediction is correct:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Ground truth of test image[0]: {}'.format(test_labels[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualize classification results\n",
    "We can define a couple of functions to visualize the classification results on the MNIST dataset with our model. First, we'll write a function, `plot_image`, to plot images along with their predicted label and the probability of the prediction. Second, we'll also define a function, `plot_value_array`, to plot the prediction probabilities for each of the digits. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_image(i, predictions_array, true_label, img):\n",
    "    predictions_array, true_label, img = predictions_array, true_label[i], img[i]\n",
    "    plt.grid(False)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "\n",
    "    plt.imshow(np.squeeze(img), cmap=plt.cm.binary)\n",
    "\n",
    "    predicted_label = np.argmax(predictions_array)\n",
    "    if predicted_label == true_label:\n",
    "        color = 'blue'\n",
    "    else:\n",
    "        color = 'red'\n",
    "\n",
    "    plt.xlabel(\"{} {:2.0f}% ({})\".format(class_names[predicted_label],\n",
    "                                100*np.max(predictions_array),\n",
    "                                class_names[true_label]),\n",
    "                                color=color)\n",
    "\n",
    "def plot_value_array(i, predictions_array, true_label):\n",
    "    predictions_array, true_label = predictions_array, true_label[i]\n",
    "    plt.grid(False)\n",
    "    plt.xticks(range(10))\n",
    "    plt.yticks([])\n",
    "    thisplot = plt.bar(range(10), predictions_array, color=\"#777777\")\n",
    "    plt.ylim([0, 1])\n",
    "    predicted_label = np.argmax(predictions_array)\n",
    "\n",
    "    thisplot[predicted_label].set_color('red')\n",
    "    thisplot[true_label].set_color('blue')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use these functions to visualize the model's predictions for the images in the test dataset: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 8 \n",
    "plt.figure(figsize=(6,3))\n",
    "plt.subplot(1, 2, 1)\n",
    "plot_image(i, predictions[i], test_labels, test_images)\n",
    "plt.subplot(1, 2, 2)\n",
    "plot_value_array(i, predictions[i], test_labels)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also plot several images along with their predictions, where correct prediction labels are blue and wrong prediction labels are red. The number below the image gives the degree of confidence for the predicted label. Note the model may be very confident in an incorrect prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the first X test images, their predicted labels, and the true labels.\n",
    "# Color correct predictions in blue and incorrect predictions in red.\n",
    "num_rows = 5\n",
    "num_cols = 3\n",
    "num_images = num_rows*num_cols\n",
    "plt.figure(figsize=(2*2*num_cols, 2*num_rows))\n",
    "for i in range(num_images):\n",
    "    plt.subplot(num_rows, 2*num_cols, 2*i+1)\n",
    "    plot_image(i, predictions[i], test_labels, test_images)\n",
    "    plt.subplot(num_rows, 2*num_cols, 2*i+2)\n",
    "    plot_value_array(i, predictions[i], test_labels)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time, let's check how the mispredicted images look like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrong_prediction=[]\n",
    "for i in range(1000):\n",
    "    if np.argmax(predictions[i]) != test_labels[i]:\n",
    "        wrong_prediction.append(i)\n",
    "\n",
    "num_rows = 5\n",
    "num_cols = 3\n",
    "num_images = num_rows*num_cols\n",
    "plt.figure(figsize=(2*2*num_cols, 2*num_rows))\n",
    "for i in range(num_images):\n",
    "    plt.subplot(num_rows, 2*num_cols, 2*i+1)\n",
    "    plot_image(wrong_prediction[i], predictions[wrong_prediction[i]], test_labels, test_images)\n",
    "    plt.subplot(num_rows, 2*num_cols, 2*i+2)\n",
    "    plot_value_array(wrong_prediction[i], predictions[wrong_prediction[i]], test_labels)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some images are difficult to distinguish even for human-beings. Through this, we can learn that in order to get high model performance, the quality of the dataset to be used for the training must be well managed."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
