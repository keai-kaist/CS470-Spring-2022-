{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS470 인공지능개론\n",
    "## Deep Learning Practice \n",
    "#### Prof. Ho-Jin Choi\n",
    "#### School of Computing, KAIST\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Introduction to Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Neural Network Basic**  \n",
    "    1-1. Linear model  \n",
    "    1-2. Multi-Layer Perceptron  \n",
    "    1-3. Deep Neural Network  \n",
    "    1-4. Training neural network\n",
    "    \n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Neural Network Basic \n",
    "\n",
    "## 1-1. Linear model \n",
    "Let's consider the binary classification.\n",
    "\n",
    "<img src=\"./imgs/bt_example.png\" align=\"center\" width=\"600\"/>\n",
    "\n",
    "To solve above problem, we can define a simple linear model as follows.\n",
    "\n",
    "<img src=\"./imgs/linear_model.png\" align=\"center\" width=\"600\"/>\n",
    "\n",
    "With the linear model, we apply it to binary classification.\n",
    "\n",
    "<img src=\"./imgs/lm_classification.png\" align=\"center\" width=\"600\"/>\n",
    "\n",
    "#### Issues of linear models\n",
    "- Most real-world data is not linearly separable\n",
    "- In other words, any linear models cannot separate regions correctly\n",
    "- Therefore, non-linearities is neceesary to model arbitrary complex functions \n",
    "\n",
    "<img src=\"./imgs/issues_lm.png\" align=\"center\" width=\"700\"/>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1-2. Multi-Layer Perceptron (MLP)\n",
    "\n",
    "#### Injecting non-linearity \n",
    "To solve above issues, we can inject the non-linearity into the linear model through the non-linear functions such as **Sigmoid, Hyperbolic Tangent(tanh), Rectified Linear (ReLU).**\n",
    "\n",
    "<img src=\"./imgs/inject_non_liearity.png\" align=\"center\" width=\"600\"/>\n",
    "\n",
    "<img src=\"./imgs/act_functions.png\" align=\"center\" width=\"600\"/>\n",
    "\n",
    "\n",
    "#### Perceptron: simplified view \n",
    "Perceptron: A Perceptron is an algorithm used for supervised learning of binary classifiers. Binary classifiers decide whether an input, usually represented by a series of vectors, belongs to a specific class. In short, a perceptron is a single-layer neural network. (Defined by Deep AI)\n",
    "\n",
    "<img src=\"./imgs/perceptrons.png\" align=\"center\" width=\"700\"/>\n",
    "\n",
    "\n",
    "#### Multi-Layer Perceptron \n",
    "MLP consists of at least three layers of nodes: an input layer, a hidden layer and an output layer. \n",
    "\n",
    "<img src=\"./imgs/mlp.png\" align=\"center\" width=\"700\"/>\n",
    "\n",
    "\n",
    "The reason why do we stack more layers is that 1) hidden layers are nonlinear embeddings of the input; 2) The model can embed the data into\n",
    "the linearly separable space.\n",
    "\n",
    "<img src=\"./imgs/mlp2.png\" align=\"center\" width=\"700\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1-3. Deep Neural Network (DNN)\n",
    "By stacking the more and more layers, neural netowrks have representing and modeling ability for given complex data.\n",
    "\n",
    "<img src=\"./imgs/dnn.png\" align=\"center\" width=\"700\"/>\n",
    "\n",
    "\n",
    "#### Example: recognizing handwritten digits\n",
    "\n",
    "Examples of data are as follows (MNIST)\n",
    "\n",
    "<img src=\"./imgs/handwritten_digits.jpeg\" align=\"center\" width=\"600\"/>\n",
    "\n",
    "\n",
    "To recognize the handwritten digits using DNN, we first preprocess the images in order to feed them to the model and then train the model.\n",
    "\n",
    "**Data representation**\n",
    "Representing a gray-scale image into an array (i.e. a vector)\n",
    "\n",
    "<img src=\"./imgs/data_representation.png\" align=\"center\" width=\"700\"/>\n",
    "\n",
    "\n",
    "**Forward propagation (Embedding viewpoint)**\n",
    "The vectorized image is propagated through the layers and classified into one of the digits, 0~9. In the case of trained DNN, each layer represents non-linear mebedding of input to easily separable space. \n",
    "\n",
    "<img src=\"./imgs/fp_mnist.png\" align=\"center\" width=\"700\"/>\n",
    "\n",
    "\n",
    "**Recognition viewpoint**\n",
    "The DNN model understands given handwritten digit by combining the abstracted features represented through multiple layers. In other words, DNN learns a hierarchy of features capturing different levels of abstractions. \n",
    "\n",
    "<img src=\"./imgs/recognition_viewpoint.png\" align=\"center\" width=\"700\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1-4. Training neural network \n",
    "\n",
    "- **Objective:** find a set of parameters that minimize the error on the dataset.\n",
    "- Notations\n",
    "    - Datasets: ${(x^{(1)},y^{(1)}),(x^{(2)},y^{(2)}), \\dots, (x^{(N)},y^{(N)})}$ for $N$ number of training data\n",
    "    - Parameters: ${\\text{w}^{(1)},\\text{w}^{(2)}, \\dots, \\text{w}^{(L)}}$ for $L$ number of layers\n",
    "    \n",
    "   \n",
    "<img src=\"./imgs/dnn_training1.png\" align=\"center\" width=\"600\"/>\n",
    "\n",
    "\n",
    "- **Loss function**\n",
    "    - Measurement on the mismatch between the model prediction and the true label.\n",
    "    - There are many ways to define the degree of mismatch (i.e. misprediction, error).\n",
    "    - Key to “quantify the performance” of the model on the specific task and data.\n",
    "    \n",
    "<img src=\"./imgs/loss_func.png\" align=\"center\" width=\"600\"/>\n",
    "    \n",
    "    \n",
    "- Example of loss function - Mean Squared Error(MSE)\n",
    "    - Regression tasks\n",
    "\n",
    "<img src=\"./imgs/mse.png\" align=\"center\" width=\"600\"/>\n",
    "  \n",
    "  \n",
    "- Example of loss function - Binary cross entropy \n",
    "    - Binary classification \n",
    "    - For predicted values, can be interpreted as a probability vector using softmax function \n",
    "\n",
    "<img src=\"./imgs/bce.png\" align=\"center\" width=\"600\"/>\n",
    "\n",
    "\n",
    "#### Optimizing the loss function (Back-propagation)\n",
    "- **Challenges** for optimizing the loss function\n",
    "    - It is highly non-convex and non-concave.\n",
    "    - It is impossible to find the analytical solution.\n",
    "    \n",
    "- **Optimiation via gradient descent**\n",
    "\n",
    "<img src=\"./imgs/gradient_descent.png\" align=\"center\" width=\"600\"/>\n",
    "\n",
    "- Algorithm (gradient descent)\n",
    "\n",
    "<img src=\"./imgs/gd_a1.png\" align=\"center\" width=\"600\"/>\n",
    "\n",
    "- Algorithm (stochastic gradient descent)\n",
    "\n",
    "<img src=\"./imgs/gd_a2.png\" align=\"center\" width=\"600\"/>\n",
    "\n",
    "- **Algorithm (minibatch stochastic gradient descent)**\n",
    "\n",
    "<img src=\"./imgs/gd_a3.png\" align=\"center\" width=\"600\"/>\n",
    "\n",
    "\n",
    "\n",
    "#### Computing gradients of weights in neural network \n",
    "- Chain rule: propagating the gradient across the layers\n",
    "    - Simplest example: two-layer neural network with one hidden node\n",
    "    - $\\hat{y}=f(x;\\text{W})$\n",
    "    \n",
    "<img src=\"./imgs/chain_rule.png\" align=\"center\" width=\"700\"/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
